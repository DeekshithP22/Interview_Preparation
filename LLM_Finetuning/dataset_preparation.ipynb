{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\interview_preparation\\.venv\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (24.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\interview_preparation\\.venv\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\interview_preparation\\.venv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\interview_preparation\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\interview_preparation\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\interview_preparation\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\interview_preparation\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.7.4)\n",
      "Requirement already satisfied: colorama in c:\\interview_preparation\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\interview_preparation\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\interview_preparation\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\interview_preparation\\.venv\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\interview_preparation\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import json\n",
    " \n",
    "def load_translation_dataset(file_path):\n",
    "    \"\"\"\n",
    "    Load and prepare the translation dataset\n",
    "    \"\"\"\n",
    "    # Read the JSON file\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    # Prepare the examples in Llama instruction format\n",
    "    formatted_data = []\n",
    "    for example in data['training_data']:\n",
    "        # Create instruction that includes preservation information\n",
    "        instruction = f\"\"\"Translate from {example['source_lang']} to {example['target_lang']}.\n",
    "Context: {example['context']}\n",
    "Preserve these words unchanged and it has to be transliterated to target langauge keeping Abbreviations as is: {', '.join(example['domain_terms'])}\n",
    " \n",
    "Text: {example['source_text']}\"\"\"\n",
    "        formatted_data.append({\n",
    "            'instruction': instruction,\n",
    "            'input': '',  # Empty as instruction contains the source text\n",
    "            'output': example['target_text']\n",
    "        })\n",
    "    # Create Hugging Face dataset\n",
    "    dataset = Dataset.from_list(formatted_data)\n",
    "    dataset.save_to_disk(r'C:\\Interview_Preparation\\LLM_Finetuning\\final_instructional_format') \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "545a70aafe6a48d7a526c843262f7aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample instruction format:\n",
      "Translate from en to ja.\n",
      "Context: dashboard_header_notification\n",
      "Preserve these words unchanged and it has to be transliterated to target langauge keeping Abbreviations as is: EY, Mobility, Pathway, GTR, APAC\n",
      " \n",
      "Text: EY Mobility Pathway Dashboard: GTR approval pending for APAC assignment\n",
      "\n",
      "Expected output:\n",
      "Bienvenido a EY Mobilidad PathFainder. Su acceso al portal GMS para la revisión de cumplimiento FSO está listo.\n"
     ]
    }
   ],
   "source": [
    " # Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the dataset\n",
    "    dataset = load_translation_dataset(r'C:\\Interview_Preparation\\LLM_Finetuning\\translation_dataset_v1.json')\n",
    "    # Print a sample\n",
    "    print(\"\\nSample instruction format:\")\n",
    "    print(dataset[0]['instruction'])\n",
    "    print(\"\\nExpected output:\")\n",
    "    print(dataset[1]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "from typing import Dict\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the Alpaca prompt format globally\n",
    "ALPACA_PROMPT = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\"\n",
    "\n",
    "def load_json_dataset(file_path: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Load JSON dataset from file path.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def convert_to_alpaca_format(examples):\n",
    "    \"\"\"\n",
    "    Convert translation dataset to Alpaca format with clear translation instructions.\n",
    "    \"\"\"\n",
    "    alpaca_format = {\n",
    "        \"instruction\": [],\n",
    "        \"input\": [],\n",
    "        \"output\": []\n",
    "    }\n",
    "    \n",
    "    for item in examples[\"training_data\"]:\n",
    "        # Create comprehensive instruction\n",
    "        instruction_parts = [\n",
    "            f\"Translate the following text from {item['source_lang']} to {item['target_lang']}\",\n",
    "            \"Translation guidelines:\",\n",
    "            \"1. Maintain the original meaning and context\",\n",
    "            \"2. Preserve sentence structure where appropriate for the target language\",\n",
    "            f\"3. Domain terms ({', '.join(item.get('domain_terms', []))}):\",\n",
    "            \"   - Transliterate non-abbreviation terms using target language characters while preserving their pronunciation\",\n",
    "            \"   - Keep abbreviations as is in the target text\"\n",
    "        ]\n",
    "        \n",
    "        instruction = \"\\n\".join(instruction_parts)\n",
    "        \n",
    "        # Prepare input with context\n",
    "        input_parts = [item[\"source_text\"]]\n",
    "        \n",
    "        context_info = []\n",
    "        if item.get(\"context\"):\n",
    "            context_info.append(f\"Context: {item['context']}\")\n",
    "        if item.get(\"content_type\"):\n",
    "            context_info.append(f\"Content type: {item['content_type']}\")\n",
    "        if item.get(\"domain_terms\"):\n",
    "            context_info.append(f\"Domain terms: {', '.join(item['domain_terms'])}\")\n",
    "            \n",
    "        if context_info:\n",
    "            input_parts.append(\"Additional information:\")\n",
    "            input_parts.extend(context_info)\n",
    "        \n",
    "        alpaca_format[\"instruction\"].append(instruction)\n",
    "        alpaca_format[\"input\"].append(\"\\n\".join(input_parts))\n",
    "        alpaca_format[\"output\"].append(item[\"target_text\"])\n",
    "    \n",
    "    return alpaca_format\n",
    "\n",
    "def process_dataset(json_path: str, output_path: str = None, eos_token: str = \"</s>\"):\n",
    "    \"\"\"\n",
    "    Process JSON dataset and convert it to Alpaca format.\n",
    "    \n",
    "    Args:\n",
    "        json_path: Path to input JSON file\n",
    "        output_path: Optional path to save processed dataset\n",
    "        eos_token: End of sequence token (default: \"</s>\")\n",
    "    \n",
    "    Returns:\n",
    "        datasets.Dataset: Processed dataset in Alpaca format\n",
    "    \"\"\"\n",
    "    # Load JSON data\n",
    "    print(f\"Loading dataset from {json_path}\")\n",
    "    data = load_json_dataset(json_path)\n",
    "    \n",
    "    # Convert to Alpaca format\n",
    "    print(\"Converting to Alpaca format\")\n",
    "    alpaca_formatted = convert_to_alpaca_format(data)\n",
    "    \n",
    "    # Create Dataset object\n",
    "    dataset = Dataset.from_dict(alpaca_formatted)\n",
    "    \n",
    "    # Apply final formatting\n",
    "    print(\"Applying final formatting\")\n",
    "    final_dataset = dataset.map(\n",
    "        lambda examples: {\n",
    "            \"text\": [\n",
    "                ALPACA_PROMPT.format(i, inp, o) + eos_token\n",
    "                for i, inp, o in zip(examples[\"instruction\"], examples[\"input\"], examples[\"output\"])\n",
    "            ]\n",
    "        },\n",
    "        batched=True\n",
    "    )\n",
    "    \n",
    "    # Save processed dataset if output path is provided\n",
    "    if output_path:\n",
    "        print(f\"Saving processed dataset to {output_path}\")\n",
    "        final_dataset.save_to_disk(output_path)\n",
    "    \n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from C:\\Interview_Preparation\\LLM_Finetuning\\translation_dataset_v1.json\n",
      "Converting to Alpaca format\n",
      "Applying final formatting\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73886228c3c7403abbd09c8fa515137a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving processed dataset to C:\\Interview_Preparation\\LLM_Finetuning\\final_instructional_format\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac5a49587fe4041887ae478defae3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample from processed dataset:\n",
      "{'instruction': 'Translate the following text from en to ja\\nTranslation guidelines:\\n1. Maintain the original meaning and context\\n2. Preserve sentence structure where appropriate for the target language\\n3. Domain terms (EY, Mobility, Pathway, GTR, APAC):\\n   - Transliterate non-abbreviation terms using target language characters while preserving their pronunciation\\n   - Keep abbreviations as is in the target text', 'input': 'EY Mobility Pathway Dashboard: GTR approval pending for APAC assignment\\nAdditional information:\\nContext: dashboard_header_notification\\nContent type: header\\nDomain terms: EY, Mobility, Pathway, GTR, APAC', 'output': 'イーワイ・モビリティ・パスウェイ ダッシュボード：APACアサインメントのGTR承認待ち', 'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nTranslate the following text from en to ja\\nTranslation guidelines:\\n1. Maintain the original meaning and context\\n2. Preserve sentence structure where appropriate for the target language\\n3. Domain terms (EY, Mobility, Pathway, GTR, APAC):\\n   - Transliterate non-abbreviation terms using target language characters while preserving their pronunciation\\n   - Keep abbreviations as is in the target text\\n\\n### Input:\\nEY Mobility Pathway Dashboard: GTR approval pending for APAC assignment\\nAdditional information:\\nContext: dashboard_header_notification\\nContent type: header\\nDomain terms: EY, Mobility, Pathway, GTR, APAC\\n\\n### Response:\\nイーワイ・モビリティ・パスウェイ ダッシュボード：APACアサインメントのGTR承認待ち</s>'}\n",
      "\n",
      "Total examples processed: 50\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Example usage\n",
    "        json_path = r\"C:\\Interview_Preparation\\LLM_Finetuning\\translation_dataset_v1.json\"\n",
    "        output_path = r\"C:\\Interview_Preparation\\LLM_Finetuning\\final_instructional_format\"  # Optional\n",
    "        \n",
    "        # Process with output saving\n",
    "        dataset = process_dataset(\n",
    "            json_path=json_path,\n",
    "            output_path=output_path,\n",
    "            eos_token=\"</s>\"  # You can change this to match your tokenizer's EOS token\n",
    "        )\n",
    "        \n",
    "        # Print sample to verify\n",
    "        print(\"\\nSample from processed dataset:\")\n",
    "        print(dataset[0])\n",
    "        \n",
    "        print(f\"\\nTotal examples processed: {len(dataset)}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file at {json_path}\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Invalid JSON format in {json_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
