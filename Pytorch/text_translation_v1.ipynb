{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "class NLLBTranslator:\n",
    "    def __init__(self, model_name: str = \"facebook/nllb-200-distilled-600M\"):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def translate(self, text: str, src_lang: str, tgt_lang: str) -> str:\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Set the language tokens\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        # Generate translation\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        return self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    def translate_batch(self, texts: List[str], src_lang: str, tgt_lang: str) -> List[str]:\n",
    "        # Tokenize the input texts\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "        # Set the language tokens\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        # Generate translations\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        # Decode the outputs\n",
    "        return [self.tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    translator = NLLBTranslator()\n",
    "\n",
    "    # Single translation\n",
    "    text = \"Hello, how are you?\"\n",
    "    src_lang = \"eng_Latn\"  # English\n",
    "    tgt_lang = \"fra_Latn\"  # French\n",
    "    translation = translator.translate(text, src_lang, tgt_lang)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "\n",
    "    # Batch translation\n",
    "    texts = [\"Hello, how are you?\", \"I love programming\", \"Machine learning is fascinating\"]\n",
    "    translations = translator.translate_batch(texts, src_lang, tgt_lang)\n",
    "    for original, translation in zip(texts, translations):\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! I'll provide you with a complete working Python code for inferencing with NLLB using CPU. This code will demonstrate how to load the model, tokenize input text, perform translation, and decode the output.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "\n",
    "class NLLBTranslator:\n",
    "    def __init__(self, model_name: str = \"facebook/nllb-200-distilled-600M\"):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "\n",
    "    def translate(self, text: str, src_lang: str, tgt_lang: str) -> str:\n",
    "        # Tokenize the input text\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "        # Set the language tokens\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        # Generate translation\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        # Decode the output\n",
    "        return self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    def translate_batch(self, texts: List[str], src_lang: str, tgt_lang: str) -> List[str]:\n",
    "        # Tokenize the input texts\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "\n",
    "        # Set the language tokens\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        # Generate translations\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        # Decode the outputs\n",
    "        return [self.tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    translator = NLLBTranslator()\n",
    "\n",
    "    # Single translation\n",
    "    text = \"Hello, how are you?\"\n",
    "    src_lang = \"eng_Latn\"  # English\n",
    "    tgt_lang = \"fra_Latn\"  # French\n",
    "    translation = translator.translate(text, src_lang, tgt_lang)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "\n",
    "    # Batch translation\n",
    "    texts = [\"Hello, how are you?\", \"I love programming\", \"Machine learning is fascinating\"]\n",
    "    translations = translator.translate_batch(texts, src_lang, tgt_lang)\n",
    "    for original, translation in zip(texts, translations):\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "        print()\n",
    "\n",
    "```\n",
    "\n",
    "This code provides a `NLLBTranslator` class that can be used for both single and batch translations using the NLLB model. Here's a breakdown of the code:\n",
    "\n",
    "1. We import the necessary libraries: `torch` for PyTorch operations, and `AutoTokenizer` and `AutoModelForSeq2SeqLM` from the `transformers` library.\n",
    "\n",
    "2. The `NLLBTranslator` class is initialized with a model name. By default, it uses the distilled 600M parameter version of NLLB, which is smaller and faster than the full model while still providing good performance.\n",
    "\n",
    "3. The `translate` method handles single text translation:\n",
    "   - It tokenizes the input text.\n",
    "   - Sets the source and target language tokens.\n",
    "   - Generates the translation using the model.\n",
    "   - Decodes the output back into text.\n",
    "\n",
    "4. The `translate_batch` method handles batch translation, which is more efficient for multiple texts.\n",
    "\n",
    "5. In the example usage section, we demonstrate both single and batch translation.\n",
    "\n",
    "To use this code:\n",
    "\n",
    "1. Make sure you have the required libraries installed:\n",
    "   ```\n",
    "   pip install torch transformers\n",
    "   ```\n",
    "\n",
    "2. Save the code to a file, e.g., `nllb_translator.py`.\n",
    "\n",
    "3. Run the script:\n",
    "   ```\n",
    "   python nllb_translator.py\n",
    "   ```\n",
    "\n",
    "Note that while this code uses CPU for inference as requested, it may be slow, especially for longer texts or larger batches. For production use with CPU, you might want to consider:\n",
    "\n",
    "- Using a smaller model variant (like the one used in this example).\n",
    "- Implementing caching mechanisms for frequent translations.\n",
    "- Setting up a queue system to manage translation requests.\n",
    "- Using batching to improve throughput.\n",
    "\n",
    "Would you like any modifications to this code or explanations about specific parts of it?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Certainly! I'll provide you with information about the different parameter sizes available for the NLLB (No Language Left Behind) model family.\n",
    "\n",
    "NLLB comes in several different sizes, each offering a trade-off between translation quality and computational requirements. Here are the main variants:\n",
    "\n",
    "1. NLLB-200 54.5B: The largest and most capable model\n",
    "   - Parameters: 54.5 billion\n",
    "   - Model name: \"facebook/nllb-200-54.5B\"\n",
    "\n",
    "2. NLLB-200 3.3B: A medium-sized model\n",
    "   - Parameters: 3.3 billion\n",
    "   - Model name: \"facebook/nllb-200-3.3B\"\n",
    "\n",
    "3. NLLB-200 1.3B: A smaller model\n",
    "   - Parameters: 1.3 billion\n",
    "   - Model name: \"facebook/nllb-200-1.3B\"\n",
    "\n",
    "4. NLLB-200 distilled 1.3B: A distilled version of the 1.3B model\n",
    "   - Parameters: 1.3 billion\n",
    "   - Model name: \"facebook/nllb-200-distilled-1.3B\"\n",
    "\n",
    "5. NLLB-200 distilled 600M: The smallest distilled model\n",
    "   - Parameters: 600 million\n",
    "   - Model name: \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "Each of these models supports translation between 200+ languages, but the larger models generally provide better translation quality, especially for low-resource languages and more complex texts.\n",
    "\n",
    "Here's a quick guide on choosing:\n",
    "\n",
    "- For highest quality and if you have powerful hardware: Use the 54.5B model\n",
    "- For a good balance of quality and speed: Consider the 3.3B model\n",
    "- For faster inference with still good quality: Try the 1.3B model\n",
    "- For resource-constrained environments: Use the distilled 600M model\n",
    "\n",
    "To use a different model in the code I provided earlier, you would simply change the model name when initializing the `NLLBTranslator` class. For example:\n",
    "\n",
    "```python\n",
    "# For the largest model\n",
    "translator = NLLBTranslator(\"facebook/nllb-200-54.5B\")\n",
    "\n",
    "# For the 3.3B model\n",
    "translator = NLLBTranslator(\"facebook/nllb-200-3.3B\")\n",
    "```\n",
    "\n",
    "Remember that larger models will require more memory and computational resources, which can significantly impact inference speed on CPU. The 54.5B model, in particular, may be impractical for CPU-only inference due to its size.\n",
    "\n",
    "For your production use case with CPU inference, the distilled 600M model (as used in the example) or the 1.3B model might be the most practical choices, depending on your specific requirements for speed vs. quality.\n",
    "\n",
    "Would you like more information on how these different model sizes might affect your implementation or performance?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "I apologize for the confusion. In the code I provided earlier, the source language is not automatically detected. Let me clarify this and provide a way to implement language detection.\n",
    "\n",
    "In the original code:\n",
    "\n",
    "1. The source language (`src_lang`) is explicitly specified when calling the `translate` or `translate_batch` methods.\n",
    "2. Both the source and target languages need to be provided as ISO 639-3 codes followed by the script code (e.g., \"eng_Latn\" for English, \"fra_Latn\" for French).\n",
    "\n",
    "NLLB itself does not have built-in automatic language detection. However, we can add this functionality by using a separate language detection library. A popular choice is `langdetect`.\n",
    "\n",
    "Here's how we can modify the `NLLBTranslator` class to include automatic language detection:\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "from langdetect import detect\n",
    "import langcodes\n",
    "\n",
    "class NLLBTranslator:\n",
    "    def __init__(self, model_name: str = \"facebook/nllb-200-distilled-600M\"):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.lang_code_map = {\n",
    "            'en': 'eng_Latn', 'fr': 'fra_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn',\n",
    "            'it': 'ita_Latn', 'pt': 'por_Latn', 'nl': 'nld_Latn', 'ru': 'rus_Cyrl',\n",
    "            'zh': 'zho_Hans', 'ja': 'jpn_Jpan', 'ko': 'kor_Hang', 'ar': 'ara_Arab',\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        try:\n",
    "            lang_code = detect(text)\n",
    "            return self.lang_code_map.get(lang_code, 'eng_Latn')  # Default to English if not found\n",
    "        except:\n",
    "            return 'eng_Latn'  # Default to English if detection fails\n",
    "\n",
    "    def translate(self, text: str, tgt_lang: str, src_lang: str = None) -> str:\n",
    "        if src_lang is None:\n",
    "            src_lang = self.detect_language(text)\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    def translate_batch(self, texts: List[str], tgt_lang: str, src_lang: str = None) -> List[str]:\n",
    "        if src_lang is None:\n",
    "            src_langs = [self.detect_language(text) for text in texts]\n",
    "        else:\n",
    "            src_langs = [src_lang] * len(texts)\n",
    "\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_langs,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        return [self.tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    translator = NLLBTranslator()\n",
    "\n",
    "    # Single translation with auto-detection\n",
    "    text = \"Hello, how are you?\"\n",
    "    tgt_lang = \"fra_Latn\"  # French\n",
    "    translation = translator.translate(text, tgt_lang)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "\n",
    "    # Batch translation with auto-detection\n",
    "    texts = [\"Hello, how are you?\", \"Bonjour, comment allez-vous?\", \"Hola, ¿cómo estás?\"]\n",
    "    translations = translator.translate_batch(texts, tgt_lang)\n",
    "    for original, translation in zip(texts, translations):\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "        print()\n",
    "\n",
    "```\n",
    "\n",
    "Key changes and additions:\n",
    "\n",
    "1. We've added the `langdetect` library for language detection.\n",
    "\n",
    "2. A `lang_code_map` dictionary is added to map ISO 639-1 codes (returned by `langdetect`) to NLLB's required format.\n",
    "\n",
    "3. A new `detect_language` method is added to detect the language and map it to NLLB's format.\n",
    "\n",
    "4. The `translate` and `translate_batch` methods are modified to use language detection when `src_lang` is not provided.\n",
    "\n",
    "5. In the example usage, we demonstrate translation with automatic language detection for both single and batch translations.\n",
    "\n",
    "To use this updated code:\n",
    "\n",
    "1. Install the additional required library:\n",
    "   ```\n",
    "   pip install langdetect\n",
    "   ```\n",
    "\n",
    "2. Save the updated code to a file, e.g., `nllb_translator_autodetect.py`.\n",
    "\n",
    "3. Run the script:\n",
    "   ```\n",
    "   python nllb_translator_autodetect.py\n",
    "   ```\n",
    "\n",
    "Note that automatic language detection is not 100% accurate, especially for short texts or languages with similar features. For critical applications, you might want to implement a fallback mechanism or allow users to specify the source language when the detection might be unreliable.\n",
    "\n",
    "Also, remember that language detection adds some overhead to the translation process. If you know the source language in advance (e.g., from user settings or input selection), it's generally more efficient to specify it explicitly.\n",
    "\n",
    "Would you like any further modifications or explanations about this updated code?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatically detect the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from typing import List\n",
    "from langdetect import detect\n",
    "import langcodes\n",
    "\n",
    "class NLLBTranslator:\n",
    "    def __init__(self, model_name: str = \"facebook/nllb-200-distilled-600M\"):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.lang_code_map = {\n",
    "            'en': 'eng_Latn', 'fr': 'fra_Latn', 'es': 'spa_Latn', 'de': 'deu_Latn',\n",
    "            'it': 'ita_Latn', 'pt': 'por_Latn', 'nl': 'nld_Latn', 'ru': 'rus_Cyrl',\n",
    "            'zh': 'zho_Hans', 'ja': 'jpn_Jpan', 'ko': 'kor_Hang', 'ar': 'ara_Arab',\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        try:\n",
    "            lang_code = detect(text)\n",
    "            return self.lang_code_map.get(lang_code, 'eng_Latn')  # Default to English if not found\n",
    "        except:\n",
    "            return 'eng_Latn'  # Default to English if detection fails\n",
    "\n",
    "    def translate(self, text: str, tgt_lang: str, src_lang: str = None) -> str:\n",
    "        if src_lang is None:\n",
    "            src_lang = self.detect_language(text)\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        src_lang_token = self.tokenizer.lang_code_to_id[src_lang]\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_lang,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        return self.tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "    def translate_batch(self, texts: List[str], tgt_lang: str, src_lang: str = None) -> List[str]:\n",
    "        if src_lang is None:\n",
    "            src_langs = [self.detect_language(text) for text in texts]\n",
    "        else:\n",
    "            src_langs = [src_lang] * len(texts)\n",
    "\n",
    "        inputs = self.tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(self.device)\n",
    "        tgt_lang_token = self.tokenizer.lang_code_to_id[tgt_lang]\n",
    "\n",
    "        translated = self.model.generate(\n",
    "            **inputs,\n",
    "            forced_bos_token_id=tgt_lang_token,\n",
    "            src_lang=src_langs,\n",
    "            tgt_lang=tgt_lang,\n",
    "            max_length=256,\n",
    "            num_beams=5,\n",
    "            length_penalty=1.0,\n",
    "        )\n",
    "\n",
    "        return [self.tokenizer.decode(t, skip_special_tokens=True) for t in translated]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    translator = NLLBTranslator()\n",
    "\n",
    "    # Single translation with auto-detection\n",
    "    text = \"Hello, how are you?\"\n",
    "    tgt_lang = \"fra_Latn\"  # French\n",
    "    translation = translator.translate(text, tgt_lang)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "\n",
    "    # Batch translation with auto-detection\n",
    "    texts = [\"Hello, how are you?\", \"Bonjour, comment allez-vous?\", \"Hola, ¿cómo estás?\"]\n",
    "    translations = translator.translate_batch(texts, tgt_lang)\n",
    "    for original, translation in zip(texts, translations):\n",
    "        print(f\"Original: {original}\")\n",
    "        print(f\"Translation: {translation}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of NLLB, M2M-100, and mBART50\n",
    "\n",
    "| Feature | NLLB | M2M-100 | mBART50 |\n",
    "|---------|------|---------|---------|\n",
    "| Developer | Meta AI | Facebook AI (Meta) | Facebook AI (Meta) |\n",
    "| Release Date | 2022 | 2020 | 2020 |\n",
    "| Languages Supported | 200+ | 100 | 50 |\n",
    "| Model Size | 54.5B parameters (largest variant) | 15.4B parameters | 611M parameters |\n",
    "| Direct Translation | Yes | Yes | No (uses English as pivot) |\n",
    "| Primary Use Case | Machine Translation | Machine Translation | Multilingual NLG and Translation |\n",
    "| Low-Resource Language Support | Excellent | Good | Limited |\n",
    "| Fine-tuning Requirements | Can be fine-tuned, but performs well without | Often benefits from fine-tuning | Often requires fine-tuning |\n",
    "| Inference Speed | Moderate (varies by model size) | Moderate | Fast |\n",
    "| BLEU Score (average) | 44.3 | 34.8 | Not directly comparable |\n",
    "| Minimum GPU RAM (inference) | 8-16GB (for smaller variants) | 16GB | 8GB |\n",
    "| Recommended GPU RAM (inference) | 32-80GB (for largest variant) | 32GB | 16GB |\n",
    "| CPU Inference | Possible but slow | Possible but slow | Feasible |\n",
    "\n",
    "## Detailed Comparison:\n",
    "\n",
    "[Previous sections remain the same]\n",
    "\n",
    "9. Hardware Requirements:\n",
    "   - NLLB: \n",
    "     * Most demanding of the three. \n",
    "     * Requires high-end GPUs for efficient inference, especially for larger variants.\n",
    "     * Minimum 8-16GB GPU RAM for smaller variants, 32-80GB for the largest.\n",
    "     * CPU inference is possible but extremely slow.\n",
    "   - M2M-100:\n",
    "     * Moderately demanding.\n",
    "     * Requires good GPUs for efficient inference.\n",
    "     * Minimum 16GB GPU RAM, 32GB recommended for better performance.\n",
    "     * CPU inference is possible but slow.\n",
    "   - mBART50:\n",
    "     * Least demanding of the three.\n",
    "     * Can run on mid-range GPUs.\n",
    "     * Minimum 8GB GPU RAM, 16GB recommended for better performance.\n",
    "     * CPU inference is feasible, though still slower than GPU.\n",
    "\n",
    "General Hardware Considerations:\n",
    "- GPU: All models benefit significantly from GPU acceleration. NVIDIA GPUs with CUDA support are typically used.\n",
    "- CPU: More powerful CPUs (high clock speed, multiple cores) can help with preprocessing and postprocessing tasks.\n",
    "- RAM: System RAM requirements depend on batch size and concurrent processing. 16GB minimum, 32GB or more recommended for production.\n",
    "- Storage: Fast SSD storage can help with model loading and data processing speeds.\n",
    "- Network: Fast, low-latency network connections are crucial for distributed setups or API-based services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using M2M-100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
