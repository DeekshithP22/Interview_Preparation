{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e13e6789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from typing import TypedDict, Annotated, List, Dict, Any\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from tavily import TavilyClient\n",
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c9819bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[Any], \"The conversation messages\"]\n",
    "    query: Annotated[str, \"The user's search query\"]\n",
    "    search_results: Annotated[Dict, \"Search results from Tavily\"]\n",
    "    final_answer: Annotated[str, \"Final answer to the user\"]\n",
    "\n",
    "class SimpleSearchAgent:\n",
    "    def __init__(self):\n",
    "        # Initialize Gemini LLM\n",
    "        self.llm = ChatGoogleGenerativeAI(\n",
    "            model=\"gemini-1.5-flash\",\n",
    "            api_key=os.getenv(\"GEMINI_API_KEY\")\n",
    "        )\n",
    "      \n",
    "        # Initialize Tavily client\n",
    "        self.tavily_client = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "        \n",
    "        # Create the graph\n",
    "        self.graph = self._create_graph()\n",
    "    \n",
    "    def _tavily_search(self, query: str) -> Dict:\n",
    "        \"\"\"Search using Tavily client\"\"\"\n",
    "        try:\n",
    "            # Use Tavily search with various options\n",
    "            response = self.tavily_client.search(\n",
    "                query=query,\n",
    "                search_depth=\"advanced\",  # or \"basic\"\n",
    "                max_results=5,\n",
    "                include_answer=True,\n",
    "                include_raw_content=False,\n",
    "                include_images=False\n",
    "            )\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"error\": f\"Search error: {str(e)}\",\n",
    "                \"results\": [],\n",
    "                \"answer\": \"\"\n",
    "            }\n",
    "    \n",
    "    def _search_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute Tavily search\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        search_results = self._tavily_search(query)\n",
    "        state[\"search_results\"] = search_results\n",
    "        return state\n",
    "    \n",
    "    def _answer_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Generate final answer using Gemini with search results\"\"\"\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        search_results = state[\"search_results\"]\n",
    "        \n",
    "        # Format search results for the prompt\n",
    "        results_text = \"\"\n",
    "        \n",
    "        # Check for errors first\n",
    "        if \"error\" in search_results:\n",
    "            results_text = f\"Search Error: {search_results['error']}\"\n",
    "        else:\n",
    "            # Add Tavily's AI answer if available\n",
    "            if search_results.get(\"answer\"):\n",
    "                results_text += f\"Tavily AI Answer: {search_results['answer']}\\n\\n\"\n",
    "            \n",
    "            # Add individual search results\n",
    "            if search_results.get(\"results\"):\n",
    "                results_text += \"Search Results:\\n\"\n",
    "                for i, result in enumerate(search_results[\"results\"], 1):\n",
    "                    results_text += f\"{i}. {result.get('title', 'No title')}\\n\"\n",
    "                    results_text += f\"   URL: {result.get('url', 'No URL')}\\n\"\n",
    "                    results_text += f\"   Content: {result.get('content', 'No content')[:300]}...\\n\"\n",
    "                    results_text += f\"   Score: {result.get('score', 'N/A')}\\n\\n\"\n",
    "        \n",
    "        # Create prompt for answer generation\n",
    "        answer_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a helpful AI assistant. Based on the search results provided, give a comprehensive and accurate answer to the user's question.\n",
    "\n",
    "                        Guidelines:\n",
    "                        1. Use the search results as your primary source of information\n",
    "                        2. Be factual and provide specific details from the search results\n",
    "                        3. If there's a Tavily AI answer, incorporate it but add additional context from other results\n",
    "                        4. Structure your answer clearly with relevant details\n",
    "                        5. Mention sources when citing specific information\n",
    "                        6. If search results are insufficient, clearly state what information is missing\n",
    "\n",
    "                        Search Results:\n",
    "                        {search_results}\"\"\"),\n",
    "                                    (\"human\", \"Question: {query}\")\n",
    "                                ])\n",
    "        \n",
    "        answer_chain = answer_prompt | self.llm\n",
    "        \n",
    "        try:\n",
    "            response = answer_chain.invoke({\n",
    "                \"query\": query,\n",
    "                \"search_results\": results_text\n",
    "            })\n",
    "            \n",
    "            state[\"final_answer\"] = response.content\n",
    "            \n",
    "            # Add AI response to messages\n",
    "            state[\"messages\"].append(AIMessage(content=response.content))\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error generating answer: {str(e)}\"\n",
    "            state[\"final_answer\"] = error_msg\n",
    "            state[\"messages\"].append(AIMessage(content=error_msg))\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _create_graph(self) -> StateGraph:\n",
    "        \"\"\"Create the simple LangGraph workflow\"\"\"\n",
    "        \n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        # Add nodes\n",
    "        workflow.add_node(\"search\", self._search_node)\n",
    "        workflow.add_node(\"answer\", self._answer_node)\n",
    "        \n",
    "        # Add edges\n",
    "        workflow.set_entry_point(\"search\")\n",
    "        workflow.add_edge(\"search\", \"answer\")\n",
    "        workflow.add_edge(\"answer\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def search(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Search and get answer\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with search results and answer\n",
    "        \"\"\"\n",
    "        \n",
    "        initial_state = {\n",
    "            \"messages\": [HumanMessage(content=query)],\n",
    "            \"query\": query,\n",
    "            \"search_results\": {},\n",
    "            \"final_answer\": \"\"\n",
    "        }\n",
    "        \n",
    "        final_state = self.graph.invoke(initial_state)\n",
    "        \n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"search_results\": final_state.get(\"search_results\", {}),\n",
    "            \"answer\": final_state.get(\"final_answer\", \"\"),\n",
    "            \"tavily_answer\": final_state.get(\"search_results\", {}).get(\"answer\", \"\"),\n",
    "            \"source_count\": len(final_state.get(\"search_results\", {}).get(\"results\", []))\n",
    "        }\n",
    "    \n",
    "    def get_raw_search(self, query: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get raw Tavily search results without LLM processing\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            \n",
    "        Returns:\n",
    "            Raw Tavily response\n",
    "        \"\"\"\n",
    "        return self._tavily_search(query)\n",
    "\n",
    "# Additional utility functions for different search types\n",
    "class TavilySearchUtils:\n",
    "    def __init__(self, api_key: str):\n",
    "        self.client = TavilyClient(api_key=api_key)\n",
    "    \n",
    "    def basic_search(self, query: str, max_results: int = 3):\n",
    "        \"\"\"Quick basic search\"\"\"\n",
    "        return self.client.search(\n",
    "            query=query,\n",
    "            search_depth=\"basic\",\n",
    "            max_results=max_results,\n",
    "            include_answer=True\n",
    "        )\n",
    "    \n",
    "    def deep_search(self, query: str, max_results: int = 10):\n",
    "        \"\"\"Comprehensive research search\"\"\"\n",
    "        return self.client.search(\n",
    "            query=query,\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=max_results,\n",
    "            include_answer=True,\n",
    "            include_raw_content=True\n",
    "        )\n",
    "    \n",
    "    def qna_search(self, query: str):\n",
    "        \"\"\"Get just the AI-generated answer\"\"\"\n",
    "        response = self.client.qna_search(query=query)\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82eaf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: I need complete details about the healthcare professional such as contact, email, specilaization, hospital associlated etc.Alessandra Beretta Spedali Civili(BS) Brescia\n",
      "============================================================\n",
      "Tavily AI Answer: Alessandra Beretta is a healthcare professional at Spedali Civili di Brescia, specializing in nephrology. Contact details are not publicly available. For inquiries, email protocollo.spedalicivilibresc...\n",
      "\n",
      "Final Answer: Based on the provided search results, Alessandra Beretta is a nephrology specialist at Spedali Civili di Brescia.  However, her contact details are not publicly available.  The general contact email for Spedali Civili di Brescia is protocollo.spedalicivilibrescia@legalmail.it [4].  This email addres...\n",
      "Sources found: 5\n",
      "\n",
      "--- Raw Tavily Response Preview ---\n",
      "Top result: Spedali Civili di Brescia - Healthcare provider - EuroBloodNet\n",
      "URL: https://eurobloodnet.eu/members/hospital/122/spedali-civili-di-brescia\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the agent\n",
    "    agent = SimpleSearchAgent()\n",
    "    \n",
    "    # Example searches\n",
    "    queries = [\n",
    "        \"I need complete details about the healthcare professional such as contact, email, specilaization, hospital associlated etc.Alessandra Beretta Spedali Civili(BS) Brescia\",\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Query: {query}\")\n",
    "        print('='*60)\n",
    "        \n",
    "        result = agent.search(query)\n",
    "        \n",
    "        print(f\"Tavily AI Answer: {result['tavily_answer'][:200]}...\" if result['tavily_answer'] else \"No direct answer\")\n",
    "        print(f\"\\nFinal Answer: {result['answer'][:300]}...\")\n",
    "        print(f\"Sources found: {result['source_count']}\")\n",
    "        \n",
    "        # Example of raw search\n",
    "        print(f\"\\n--- Raw Tavily Response Preview ---\")\n",
    "        raw_result = agent.get_raw_search(query)\n",
    "        if raw_result.get(\"results\"):\n",
    "            first_result = raw_result[\"results\"][0]\n",
    "            print(f\"Top result: {first_result.get('title', 'No title')}\")\n",
    "            print(f\"URL: {first_result.get('url', 'No URL')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab744d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
