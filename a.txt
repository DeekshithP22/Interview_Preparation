Here are the specific solutions for each error:

## **Solutions:**

### **1. Fix Collection[str] indexing errors (lines 392, 488, 421):**

**Problem:** `name_parts = professional_name.lower().split()` 
**Solution:** Add explicit type annotation:
```python
name_parts: List[str] = professional_name.lower().split()
```

**Or use type casting:**
```python
from typing import cast, List
name_parts = cast(List[str], professional_name.lower().split())
```

### **2. Fix type mismatch error (line 432):**

**Problem:** `self._format_search_results_for_llm(response["search_results"])`
**Solution:** Add type annotation to the method parameter:

```python
def _format_search_results_for_llm(self, search_results: Dict[str, Any]) -> str:



















```

**And ensure the response structure is properly typed:**
```python
# At the top, add proper imports
from typing import Dict, List, Any, Optional

# When calling the method, ensure type consistency
formatted_results_for_llm = self._format_search_results_for_llm(
    response["search_results"]  # This should match Dict[str, Any]
)
```

### **3. Additional fixes needed:**

**Add missing imports at the top:**
```python
from typing import List, Dict, Any, Optional, cast
```

**Fix any None handling:**
```python
# Instead of direct access, use safe access
results = search_results.get("results", [])
if results:
    # process results
```

These changes will satisfy mypy's type checking requirements.





# Simplified Pydantic model only for LLM response
class LLMSummaryResponse(BaseModel):
    """Model for LLM to provide comprehensive summary from search results"""
    llm_final_answer: str = Field(description="Comprehensive summary analyzing all search results with main search, linkedin, and workplace summaries")


class HealthcareSearchAgent:
    """Agent for searching healthcare professionals using Tavily API"""
    
    def __init__(self, tavily_api_key: Optional[str] = None, google_api_key: Optional[str] = None):
        """
        Initialize the search agent
        
        Args:
            tavily_api_key: Tavily API key (defaults to env variable)
            google_api_key: Google API key for Gemini (defaults to env variable)
        """
        # Initialize Tavily client
        self.tavily_api_key = tavily_api_key or os.getenv("TAVILY_API_KEY")
        if not self.tavily_api_key:
            raise ValueError("Tavily API key not provided. Set TAVILY_API_KEY environment variable.")
        
        self.tavily_client = TavilyClient(api_key=self.tavily_api_key)
        
        # Initialize LLM
        self.google_api_key = google_api_key or os.getenv("GEMINI_API_KEY")
        if not self.google_api_key:
            raise ValueError("Google API key not provided. Set GEMINI_API_KEY environment variable.")
        
        self.llm = ChatGoogleGenerativeAI(
            model="gemini-1.5-flash",
            api_key=self.google_api_key,
            max_output_tokens=8192,
            temperature=0.1
        ).with_structured_output(LLMSummaryResponse)
        
        # Create prompt template
        self.prompt_template = ChatPromptTemplate.from_messages([
            ("system", """You are a helpful AI assistant specialized in analyzing healthcare professional search results.
            Based on the search results provided, provide a comprehensive summary that includes:
            
            1. **Main Search Summary:** Analysis of official records, academic documents, and institutional information
            2. **LinkedIn Profile Summary:** Professional profile information and career details
            3. **Workplace Search Summary:** Additional workplace directories and institutional connections
            
            Guidelines:
            - Provide clear, factual summaries for each section
            - Highlight key qualifications, positions, and affiliations
            - Note any gaps in contact information or details
            - Keep each section concise but informative
            - If a search type has no relevant results, mention that briefly
            
            Geographic Region: {geographic_region}
            
            Search Results:
            {search_results}
            """),
            ("human", "Query for: Dr {firstName} {lastName} at {workplaceName}")
        ])
        
        # Country code mapping
        self.country_mapping = {
            "IT": "Italy",
            "US": "United States", 
            "UK": "United Kingdom",
            "DE": "Germany",
            "FR": "France",
            "ES": "Spain",
            "IN": "India",
            "CA": "Canada",
            "AU": "Australia"
        }
        
        logger.info("HealthcareSearchAgent initialized successfully")
    
    def _get_country_name(self, geographic_region: str) -> str:
        """Convert geographic region code to country name"""
        return self.country_mapping.get(geographic_region.upper(), geographic_region)
    
    def _construct_main_search_query(self, doctor_info: Dict[str, Any]) -> str:
        """Construct comprehensive main search query"""
        first_name = doctor_info.get("firstName", "")
        last_name = doctor_info.get("lastName", "") 
        workplace = doctor_info.get("workplaceName", "")
        address = doctor_info.get("address", "")
        geographic_region = doctor_info.get("geographic_region", "")
        
        # Comprehensive search targeting official records, profiles, and contact info
        main_query = f'Dr "{first_name} {last_name}" "{workplace}" {address} doctor profile contact details phone email curriculum vitae'
        
        logger.info(f"Main search query: {main_query}")
        return main_query
    
    def _construct_linkedin_query(self, doctor_info: Dict[str, Any]) -> str:
        """Construct highly targeted LinkedIn profile search"""
        first_name = doctor_info.get("firstName", "")
        last_name = doctor_info.get("lastName", "")
        workplace = doctor_info.get("workplaceName", "")
        address = doctor_info.get("address", "")
        
        # Target LinkedIn profiles specifically - using exact name matching and workplace
        linkedin_query = f'site:linkedin.com/in/ "{first_name} {last_name}" "{workplace}" {address} MD doctor physician'
        
        logger.info(f"LinkedIn search query: {linkedin_query}")
        return linkedin_query
    
    def _construct_workplace_query(self, doctor_info: Dict[str, Any]) -> str:
        """Construct workplace-specific search to find doctor in institutional websites"""
        first_name = doctor_info.get("firstName", "")
        last_name = doctor_info.get("lastName", "")
        workplace = doctor_info.get("workplaceName", "")
        address = doctor_info.get("address", "")
        
        # Target institutional websites - medical centers, hospitals, university websites
        # Use terms that appear in staff directories, faculty pages, doctor profiles
        workplace_query = f'"{first_name} {last_name}" "{workplace}" {address} (staff OR faculty OR directory OR team OR doctors OR physicians OR profile OR biography)'
        
        logger.info(f"Workplace search query: {workplace_query}")
        return workplace_query
    
    async def _search_tavily_main(self, doctor_info: Dict[str, Any], max_results: int = 10) -> Dict[str, Any]:
        """
        Perform main Tavily search with structured doctor information
        """
        try:
            main_query = self._construct_main_search_query(doctor_info)
            country = self._get_country_name(doctor_info.get("geographic_region", ""))
            
            logger.info(f"Performing main search: {main_query}")
            
            response = await asyncio.to_thread(
                self.tavily_client.search,
                query=main_query,
                search_depth="advanced",
                max_results=max_results,
                include_answer=True,
                include_raw_content=True,
                country=country,
                time_range="year",
                chunks_per_source=2,
            )
            
            logger.info(f"Main search completed. Found {len(response.get('results', []))} results")
            return response
            
        except Exception as e:
            logger.error(f"Error in main Tavily search: {str(e)}")
            return {"results": [], "answer": ""}
        
    async def _search_linkedin_profile(self, doctor_info: Dict[str, Any]) -> Dict[str, Any]:
        """Search specifically for LinkedIn profile using structured info"""
        try:
            linkedin_query = self._construct_linkedin_query(doctor_info)
            country = self._get_country_name(doctor_info.get("geographic_region", ""))
            
            logger.info(f"Performing LinkedIn search: {linkedin_query}")
            
            response = await asyncio.to_thread(
                self.tavily_client.search,
                query=linkedin_query,
                search_depth="advanced",
                max_results=3,
                include_answer=True,
                include_raw_content=True,
                country=country,
                chunks_per_source=3,
                time_range="year",
                include_domains=["linkedin.com"]
            )
            
            # Filter results to ensure relevance
            response = self._filter_relevant_results(
                response, 
                "linkedin", 
                f"{doctor_info.get('firstName', '')} {doctor_info.get('lastName', '')}"
            )
            
            logger.info(f"LinkedIn search completed. Found {len(response.get('results', []))} results")
            return response
            
        except Exception as e:
            logger.error(f"Error in LinkedIn search: {str(e)}")
            return {"results": [], "answer": ""}

    async def _search_workplace_website(self, doctor_info: Dict[str, Any]) -> Dict[str, Any]:
        """Search for doctor's information on workplace/institutional websites"""
        try:
            workplace_query = self._construct_workplace_query(doctor_info)
            country = self._get_country_name(doctor_info.get("geographic_region", ""))
            
            logger.info(f"Performing workplace search: {workplace_query}")
            
            response = await asyncio.to_thread(
                self.tavily_client.search,
                query=workplace_query,
                search_depth="advanced",
                max_results=5,
                include_answer=True,
                include_raw_content=True,
                country=country,
                time_range="year",
                chunks_per_source=2,
            )
            
            # Filter results to ensure relevance
            response = self._filter_relevant_results(
                response,
                "workplace", 
                f"{doctor_info.get('firstName', '')} {doctor_info.get('lastName', '')}"
            )
            
            logger.info(f"Workplace search completed. Found {len(response.get('results', []))} results")
            return response
            
        except Exception as e:
            logger.error(f"Error in workplace website search: {str(e)}") 
            return {"results": [], "answer": ""}
        
    
    def _extract_urls_from_results(self, search_results: Dict[str, Any]) -> List[str]:
        """Extract all URLs from search results and remove duplicates"""
        urls = []
        
        try:
            # Extract from main results
            main_results = search_results.get("main_search", {})
            if "results" in main_results:
                for result in main_results["results"]:
                    if "url" in result and result["url"]:
                        urls.append(result["url"])
            
            # Extract from LinkedIn results
            linkedin_results = search_results.get("linkedin_search", {})
            if "results" in linkedin_results:
                for result in linkedin_results["results"]:
                    if "url" in result and result["url"]:
                        urls.append(result["url"])
            
            # Extract from workplace results
            workplace_results = search_results.get("workplace_search", {})
            if "results" in workplace_results:
                for result in workplace_results["results"]:
                    if "url" in result and result["url"]:
                        urls.append(result["url"])
            
            # Remove duplicates while preserving order
            seen = set()
            unique_urls = []
            for url in urls:
                if url not in seen:
                    seen.add(url)
                    unique_urls.append(url)
            
            return unique_urls
            
        except Exception as e:
            logger.error(f"Error extracting URLs: {str(e)}")
            return []
    
    
    def _format_search_results_for_llm(self, search_results: Dict[str, Any]) -> str:
        """Format search results for LLM processing"""
        formatted_results = []
        
        try:
            # Add main search results
            main_results = search_results.get("main_search", {})
            formatted_results.append("=== MAIN SEARCH RESULTS ===")
            
            if main_results.get("results"):
                for i, result in enumerate(main_results["results"], 1):
                    formatted_results.append(f"Result {i}:")
                    formatted_results.append(f"Title: {result.get('title', 'No title')}")
                    formatted_results.append(f"URL: {result.get('url', 'No URL')}")
                    formatted_results.append(f"Content: {result.get('content', 'No content')[:1000]}...")
                    
                    if result.get("raw_content"):
                        formatted_results.append(f"Additional Details: {result['raw_content'][:1000]}...")
                    
                    formatted_results.append(f"Score: {result.get('score', 'N/A')}")
                    formatted_results.append("-" * 50)
            else:
                formatted_results.append("No main search results found.")
            
            # Add LinkedIn results if available
            linkedin_results = search_results.get("linkedin_search", {})
            if linkedin_results.get("results"):
                formatted_results.append("\n=== LINKEDIN PROFILE RESULTS ===")
                for i, result in enumerate(linkedin_results["results"], 1):
                    formatted_results.append(f"LinkedIn Result {i}:")
                    formatted_results.append(f"Title: {result.get('title', 'No title')}")
                    formatted_results.append(f"URL: {result.get('url', 'No URL')}")
                    formatted_results.append(f"Content: {result.get('content', 'No content')[:500]}...")
                    formatted_results.append("-" * 30)
            else:
                formatted_results.append("\n=== LINKEDIN PROFILE RESULTS ===")
                formatted_results.append("No relevant LinkedIn profiles found.")
            
            # Add workplace website results if available  
            workplace_results = search_results.get("workplace_search", {})
            if workplace_results.get("results"):
                formatted_results.append("\n=== WORKPLACE WEBSITE RESULTS ===")
                for i, result in enumerate(workplace_results["results"], 1):
                    formatted_results.append(f"Workplace Result {i}:")
                    formatted_results.append(f"Title: {result.get('title', 'No title')}")
                    formatted_results.append(f"URL: {result.get('url', 'No URL')}")
                    formatted_results.append(f"Content: {result.get('content', 'No content')[:500]}...")
                    formatted_results.append("-" * 30)
            else:
                formatted_results.append("\n=== WORKPLACE WEBSITE RESULTS ===")
                formatted_results.append("No workplace directory results found.")
            
            return "\n".join(formatted_results)
            
        except Exception as e:
            logger.error(f"Error formatting search results: {str(e)}")
            return str(search_results)
    
    
    async def search(self, doctor_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Search for healthcare professionals based on structured input
        
        Args:
            doctor_info: Dictionary containing structured doctor information
                        Expected keys: firstName, lastName, workplaceName, address, geographic_region
            
        Returns:
            Dictionary with search_results, tavily_answer, and llm_answer
            
        Raises:
            Exception: If search or processing fails
        """
        try:
            first_name = doctor_info.get("firstName", "")
            last_name = doctor_info.get("lastName", "")
            workplace = doctor_info.get("workplaceName", "")
            
            logger.info(f"Starting search for: Dr {first_name} {last_name} at {workplace}")
            
            # Initialize response structure
            response = {
                "search_results": {
                    "main_search": {"results": []},
                    "linkedin_search": {"results": []},
                    "workplace_search": {"results": []}
                },
                "tavily_answer": "",
                "llm_answer": ""
            }
            
            # Perform main search
            main_results = await self._search_tavily_main(doctor_info)
            
            # Extract and format main search results
            response["search_results"]["main_search"]["results"] = [
                {
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", ""),
                    "score": result.get("score", 0.0)
                } for result in main_results.get("results", [])
            ]
            
            # Extract Tavily answer from main search
            response["tavily_answer"] = main_results.get("answer", "")
            
            # Perform LinkedIn search
            linkedin_results = await self._search_linkedin_profile(doctor_info)
            
            response["search_results"]["linkedin_search"]["results"] = [
                {
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", ""),
                    "score": result.get("score", 0.0)
                } for result in linkedin_results.get("results", [])
            ]
            
            # Perform workplace search
            workplace_results = await self._search_workplace_website(doctor_info)
            
            response["search_results"]["workplace_search"]["results"] = [
                {
                    "url": result.get("url", ""),
                    "title": result.get("title", ""),
                    "content": result.get("content", ""),
                    "score": result.get("score", 0.0)
                } for result in workplace_results.get("results", [])
            ]
            
            # Format search results for LLM
            formatted_results_for_llm = self._format_search_results_for_llm(response["search_results"])
            
            # Get LLM analysis
            try:
                messages = self.prompt_template.format_messages(
                    geographic_region=self._get_country_name(doctor_info.get("geographic_region", "")),
                    search_results=formatted_results_for_llm,
                    firstName=first_name,
                    lastName=last_name,
                    workplaceName=workplace
                )
                
                logger.info("Processing results with LLM...")
                llm_response = await asyncio.to_thread(self.llm.invoke, messages)
                response["llm_answer"] = llm_response.llm_final_answer
                
            except Exception as llm_error:
                logger.error(f"Error in LLM processing: {str(llm_error)}")
                response["llm_answer"] = f"Error processing search results with AI analysis: {str(llm_error)}"
            
            logger.info(f"Search completed successfully for Dr {first_name} {last_name}")
            return response
            
        except Exception as e:
            logger.error(f"Error in search operation: {str(e)}")
            # Return error response
            return {
                "search_results": {
                    "main_search": {"results": []},
                    "linkedin_search": {"results": []},
                    "workplace_search": {"results": []}
                },
                "tavily_answer": "",
                "llm_answer": f"Search operation failed: {str(e)}"
            }
            
    def _filter_relevant_results(self, results: Dict[str, Any], search_type: str, professional_name: str) -> Dict[str, Any]:
        """Filter only relevant results based on search type"""
        if not results.get("results"):
            return results
        
        filtered_results = []
        name_parts = professional_name.lower().split()
        
        for result in results["results"]:
            content = (result.get("content", "") + result.get("title", "")).lower()
            
            # Check if the professional's name appears in the content
            if any(name_part in content for name_part in name_parts):
                filtered_results.append(result)
                if len(filtered_results) >= 3:  # Keep top 3 relevant results
                    break
        
        results["results"] = filtered_results
        return results


# Convenience function for direct import and use
def search_healthcare_professionals(
    doctor_info: Dict[str, Any],
    tavily_api_key: Optional[str] = None,
    google_api_key: Optional[str] = None
) -> Dict[str, Any]:
    """
    Convenience function to search for healthcare professionals
    
    Args:
        doctor_info: Structured dictionary with doctor information
        tavily_api_key: Optional Tavily API key
        google_api_key: Optional Google API key
        
    Returns:
        Dictionary with search results, tavily answer, and llm analysis
    """
    agent = HealthcareSearchAgent(tavily_api_key, google_api_key)
    return agent.search(doctor_info)














import logging
from logging.handlers import RotatingFileHandler
import os

# Create logs directory if it doesn't exist
os.makedirs('logs', exist_ok=True)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler(
            'logs/search_agent.log',
            maxBytes=10*1024*1024,  # 10MB per file
            backupCount=5           # Keep 5 backup files
        ),
        logging.StreamHandler()     # Also show in terminal
    ]
)





    import subprocess; print("Chrome check:", subprocess.run(['which', 'google-chrome'], capture_output=True, text=True).stdout)












def search(search_input):
    try:
        print("=== WebDriver Manager Debug ===")
        from webdriver_manager.chrome import ChromeDriverManager
        
        # Test ChromeDriver download
        print("Attempting ChromeDriver download...")
        driver_path = ChromeDriverManager().install()
        print(f"✅ ChromeDriver downloaded to: {driver_path}")
        
        # Test Chrome browser availability
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        
        print("Testing Chrome browser...")
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        
        driver = webdriver.Chrome(driver_path, options=chrome_options)
        driver.get("https://www.google.com")
        print(f"✅ Chrome browser working! Title: {driver.title}")
        driver.quit()
        
        return {"debug": "both_working"}
        
    except Exception as e:
        print(f"❌ Error: {str(e)}")
        return {"debug": "failed", "error": str(e)}










def search(search_input):
    try:
        print("=== WebDriver Manager Debug ===")
        from webdriver_manager.chrome import ChromeDriverManager
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.chrome.service import Service
        
        print("Step 1: Getting ChromeDriver...")
        driver_path = ChromeDriverManager().install()
        print(f"✅ ChromeDriver downloaded to: {driver_path}")
        
        print("Step 2: Creating Chrome options...")
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        print("✅ Chrome options created")
        
        print("Step 3: Creating Chrome service...")
        service = Service(driver_path)
        print("✅ Chrome service created")
        
        print("Step 4: Starting Chrome browser...")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        print("✅ Chrome browser started successfully")
        
        print("Step 5: Testing navigation to Google...")
        driver.get("https://www.google.com")
        page_title = driver.title
        print(f"✅ Navigation successful! Page title: {page_title}")
        
        print("Step 6: Closing browser...")
        driver.quit()
        print("✅ Browser closed successfully")
        
        print("=== All Selenium steps completed successfully ===")
        return {
            "debug": "selenium_fully_working",
            "driver_path": driver_path,
            "test_title": page_title,
            "status": "success"
        }
        
    except Exception as e:
        print(f"❌ Selenium failed with error: {str(e)}")
        import traceback
        print("Full traceback:")
        print(traceback.format_exc())
        return {
            "debug": "selenium_failed", 
            "error": str(e),
            "status": "failed"
        }












# In your master script's search function, find this section:
# 2. Execute each Python script sequentially
print("--- Starting Script Execution ---")
for script in scripts_to_run:
    script_path = os.path.join(self.scripts_folder, script)
    print(f"Running {script_path}...")
    try:
        process = await asyncio.create_subprocess_exec(
            sys.executable,
            script_path,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        stdout, stderr = await process.communicate()
        
        # ADD THESE DEBUG LINES:
        print(f"=== DEBUG: {script} ===")
        print(f"STDOUT: {stdout.decode().strip()}")
        print(f"STDERR: {stderr.decode().strip()}")
        print(f"Return code: {process.returncode}")
        print(f"=== END DEBUG ===")
        
        if process.returncode != 0:
            # Keep your existing error handling








ENV PYTHONPATH=/deps/backend:/deps/backend/app:$PYTHONPATH



print(f"=== ENVIRONMENT DEBUG ===")
print(f"Current working directory: {os.getcwd()}")
print(f"Scripts folder path: {self.scripts_folder}")
print(f"Scripts folder exists: {os.path.exists(self.scripts_folder)}")
if os.path.exists(self.scripts_folder):
    print(f"Files in scripts folder: {os.listdir(self.scripts_folder)}")
print(f"Config path: {self.config_path}")
print(f"Config exists: {os.path.exists(self.config_path)}")
print(f"Scripts to run: {scripts_to_run}")
print(f"=== END DEBUG ===")





stdout, stderr = await process.communicate()

# ENHANCED DEBUG OUTPUT
print(f"=== DEBUG: {script} ===")
print(f"Return code: {process.returncode}")
print(f"STDOUT ({len(stdout)} bytes):")
if stdout.strip():
    print(stdout.decode().strip())
else:
    print("(No stdout output)")
    
print(f"STDERR ({len(stderr)} bytes):")
if stderr.strip():
    print("ERROR DETAILS:")
    print(stderr.decode().strip())
else:
    print("(No stderr output)")
print(f"=== END DEBUG: {script} ===")




[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["."]
include = ["app*"]








    except FileNotFoundError:
        print(f"Error: The script '{script_path}' was not found.")
    except Exception as e:
        print(f"SUBPROCESS CREATION FAILED for {script}: {str(e)}")
        print(f"Exception type: {type(e).__name__}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")




# FIXED CODE (works everywhere):
import subprocess
result = subprocess.run(
    [sys.executable, script_path],
    capture_output=True,
    text=True,
    timeout=300  # 5 minute timeout per script
)
stdout = result.stdout.encode()  # Convert to bytes for compatibility
stderr = result.stderr.encode()
process = result  # For returncode compatibility



# Add this RIGHT before the for loop
print(f"BEFORE FOR LOOP: scripts_to_run = {scripts_to_run}")
print(f"Number of scripts to run: {len(scripts_to_run)}")

for script in scripts_to_run:
    print(f"LOOP START: Processing script: {script}")  # Add this
    script_path = os.path.join(self.scripts_folder, script)
    print(f"SCRIPT PATH: {script_path}")  # Add this
    print(f"Running {script_path}...")
    try:
        print(f"BEFORE SUBPROCESS: About to start {script}")  # Add this
        process = await asyncio.create_subprocess_exec(
            sys.executable,
            script_path,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
        )
        print(f"AFTER SUBPROCESS CREATION: {script}")  # Add this
        stdout, stderr = await process.communicate()
        print(f"AFTER COMMUNICATE: {script}")  # Add this
        
        # Your debug output here...
        print(f"=== DEBUG: {script} ===")
        # ... rest of debug code
        
    except Exception as e:
        print(f"EXCEPTION in subprocess: {script} - {str(e)}")  # Add this



[tool.setuptools.packages.find]
where = ["."]
include = ["app*"]

    except FileNotFoundError:
        print(f"Error: The script '{script_path}' was not found.")
    except Exception as e:
        print(f"SUBPROCESS CREATION FAILED for {script}: {str(e)}")
        print(f"Exception type: {type(e).__name__}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")






# FIXED CODE (works everywhere):
import subprocess
result = subprocess.run(
    [sys.executable, script_path],
    capture_output=True,
    text=True,
    timeout=300  # 5 minute timeout per script
)
stdout = result.stdout.encode()  # Convert to bytes for compatibility
stderr = result.stderr.encode()
process = result  # For returncode compatibility


RUN ls -la /deps/backend/app/my_agent/
RUN python -c "import sys; sys.path.append('/deps/backend'); from app.my_agent.agent import graph; print('Import successful')"




ENV PYTHONPATH=/deps/backend:/deps/backend/app:$PYTHONPATH





# BUILD SYSTEM - Makes 'app' directory a proper package
[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

# Tell setuptools to include the 'app' directory as a package
[tool.setuptools.packages.find]
where = ["."]
include = ["app*"]







[tool.setuptools.packages.find]
where = ["."]
include = ["app*"]














[tool.poetry]
name = "app"
version = "0.1.0"
description = ""
packages = [
    {include = "app"},
    {include = "app/my_agent"},
    {include = "app/core"}, 
    {include = "app/api"}
]


















[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["."]
include = ["app*"]
















RUN apt-get update && apt-get install -y \
    wget curl unzip gnupg2 software-properties-common \
    fonts-liberation libappindicator3-1 libasound2 \
    xdg-utils libatk-bridge2.0-0 libgtk-3-0 libnss3 \
    libxss1 libxtst6 ca-certificates --no-install-recommends && \
    curl -fsSL https://dl.google.com/linux/linux_signing_key.pub | apt-key add - && \
    echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" \
        > /etc/apt/sources.list.d/google-chrome.list && \
    apt-get update && apt-get install -y google-chrome-stable && \














# Install dependencies (required by Chrome)
apt-get update && apt-get install -y \
  wget unzip curl gnupg \
  libglib2.0-0 libnss3 libgconf-2-4 libfontconfig1 libx11-xcb1 \
  libxcomposite1 libxcursor1 libxdamage1 libxext6 libxi6 libxrandr2 \
  libxss1 libxtst6 libappindicator3-1 libasound2 libatk-bridge2.0-0 \
  libatk1.0-0 libcups2 libdbus-1-3 libgtk-3-0 libx11-data \
  && rm -rf /var/lib/apt/lists/*

# -----------------------------
# Install Google Chrome 138
# -----------------------------
cd /tmp && \
wget https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.92/linux64/chrome-linux64.zip && \
unzip chrome-linux64.zip

# Move chrome binary to /usr/local/bin
mv chrome-linux64/chrome /usr/local/bin/google-chrome
chmod +x /usr/local/bin/google-chrome

# Clean up
rm -rf chrome-linux64 chrome-linux64.zip

# -----------------------------
# Install ChromeDriver 138
# -----------------------------
cd /tmp && \
wget https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.92/linux64/chromedriver-linux64.zip && \
unzip chromedriver-linux64.zip

# Move chromedriver to /usr/local/bin
mv chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
chmod +x /usr/local/bin/chromedriver

# Clean up
rm -rf chromedriver-linux64 chromedriver-linux64.zip

    rm -rf /var/lib/apt/lists/*













FROM python:3.10-slim

# Install Chrome dependencies
RUN apt-get update && apt-get install -y \
    wget unzip curl gnupg \
    libglib2.0-0 libnss3 libgconf-2-4 libfontconfig1 libx11-xcb1 \
    libxcomposite1 libxcursor1 libxdamage1 libxext6 libxi6 libxrandr2 \
    libxss1 libxtst6 libappindicator3-1 libasound2 libatk-bridge2.0-0 \
    libatk1.0-0 libcups2 libdbus-1-3 libgtk-3-0 libx11-data \
    && rm -rf /var/lib/apt/lists/*

# Install Google Chrome to /usr/local/bin
RUN cd /tmp && \
    wget https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.92/linux64/chrome-linux64.zip && \
    unzip chrome-linux64.zip && \
    mv chrome-linux64/chrome /usr/local/bin/google-chrome && \
    chmod +x /usr/local/bin/google-chrome && \
    rm -rf chrome-linux64 chrome-linux64.zip

# Install ChromeDriver to /usr/local/bin
RUN cd /tmp && \
    wget https://storage.googleapis.com/chrome-for-testing-public/138.0.7204.92/linux64/chromedriver-linux64.zip && \
    unzip chromedriver-linux64.zip && \
    mv chromedriver-linux64/chromedriver /usr/local/bin/chromedriver && \
    chmod +x /usr/local/bin/chromedriver && \
    rm -rf chromedriver-linux64 chromedriver-linux64.zip

# Optional environment setup
ENV DISPLAY=:99
CMD ["python3"]





















# Alternative approach - create agent and use it
agent = WebSearchAgent()


hcp_search_input = {
    "entity_type": "ent_activity",
    "verification_needed": True,
    "geographic_region": "IT",
    "firstName": "Marcello",
    "lastName": "Marchetti",
    "workplaceName": "Fondazione IRCCS Istituto Neurologico Carlo Besta",
    "address": "Milano",
    "specialtyCode": "18"
}


workplace_search_input = {
    "entity_type": "ent_workplace",
    "verification_needed": True,
    "geographic_region": "IT",
    "workplaceName": "Fondazione IRCCS Istituto Neurologico Carlo Besta",
    "address": "Milano"
}

# Direct search
result = await agent.search(hcp_search_input)
print("Search completed!")
print(result)


















ASYNC EXAMPLE USAGE
async def example_usage_async():
    """Async example of how to use the enhanced search agent"""
    
    # HCP Search Example
    hcp_search_input = {
        "entity_typ#e": "ent_activity",
        "verification_needed": True,
        "geographic_region": "IT",
        "firstName": "Marcello",
        "lastName": "Marchetti",
        "workplaceName": "Fondazione IRCCS Istituto Neurologico Carlo Besta",
        "address": "Milano",
        "specialtyCode": "18"
    }
    
    # Workplace Search Example
    workplace_search_input = {
        "entity_type": "ent_workplace",
        "verification_needed": True,
        "geographic_region": "IT",
        "workplaceName": "Fondazione IRCCS Istituto Neurologico Carlo Besta",
        "address": "Milano"
    }
    
    # Initialize agent
    agent = WebSearchAgent()
    
    # Perform async searches concurrently
    hcp_task = agent.search(hcp_search_input)
    workplace_task = agent.search(workplace_search_input)
    
    # Wait for both searches to complete
    hcp_results, workplace_results = await asyncio.gather(hcp_task, workplace_task)
    
    return hcp_results, workplace_results















# MAIN EXECUTION
if __name__ == "__main__":
    async def main():
        try:
            start_time = asyncio.get_event_loop().time()
            hcp_results, workplace_results = await example_usage_async()
            end_time = asyncio.get_event_loop().time()
            
            print("ASYNC HCP Search completed successfully")
            print("ASYNC Workplace Search completed successfully")
            print(f"Total execution time: {end_time - start_time:.2f} seconds")
            print(f"HCP Results: {len(hcp_results['search_results']['online_search']['results'])} main results")
            print(f"Workplace Results: {len(workplace_results['search_results']['workplace_validation']['results'])} validation results")
        except Exception as e:
            print(f"Error in async example usage: {str(e)}")
            logger.error(f"Error in async example usage: {str(e)}")
    
    asyncio.run(main())
















# Add validated workplace results - ONLY FROM OFFICIAL DOMAIN
if official_domain:
    # Filter results to only include official workplace domain URLs
    for result in validated_workplace:
        validation = result.get("validation", {})
        if validation.get("is_valid", False):
            # Extract domain from result URL
            result_url = result.get("url", "")
            if result_url:
                result_domain = urlparse(result_url).netloc
                # Remove www. prefix if present
                if result_domain.startswith("www."):
                    result_domain = result_domain[4:]
                
                # Only add if result is from official workplace domain
                if result_domain == official_domain or result_domain.endswith(f".{official_domain}"):
                    clean_response["search_results"]["workplace_search"]["results"].append({
                        "url": result.get("url", ""),
                        "title": result.get("title", ""),
                        "content": result.get("content", ""),
                        "score": result.get("score", 0.0)
                    })
                    logger.info(f"Added official workplace URL: {result_url}")
                else:
                    logger.info(f"Rejected non-official URL: {result_url} (not from {official_domain})")
else:
    # If no official domain found, don't add any workplace results
    logger.warning("No official domain found - no workplace results added for safety")
