{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Translation using M2M-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "from typing import List\n",
    "from langdetect import detect\n",
    "\n",
    "class M2M100Translator:\n",
    "    def __init__(self, model_name: str = \"facebook/m2m100_418M\"):\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "        self.model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        self.lang_code_map = {\n",
    "            'en': 'en', 'fr': 'fr', 'es': 'es', 'de': 'de', 'it': 'it', 'pt': 'pt', \n",
    "            'nl': 'nl', 'ru': 'ru', 'zh': 'zh', 'ja': 'ja', 'ko': 'ko', 'ar': 'ar',\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "\n",
    "    def detect_language(self, text: str) -> str:\n",
    "        try:\n",
    "            lang_code = detect(text)\n",
    "            return self.lang_code_map.get(lang_code, 'en')  # Default to English if not found\n",
    "        except:\n",
    "            return 'en'  # Default to English if detection fails\n",
    "\n",
    "    def translate(self, text: str, tgt_lang: str, src_lang: str = None) -> str:\n",
    "        if src_lang is None:\n",
    "            src_lang = self.detect_language(text)\n",
    "        \n",
    "        self.tokenizer.src_lang = src_lang\n",
    "        encoded = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        generated_tokens = self.model.generate(\n",
    "            **encoded,\n",
    "            forced_bos_token_id=self.tokenizer.get_lang_id(tgt_lang),\n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]\n",
    "\n",
    "    def translate_batch(self, texts: List[str], tgt_lang: str, src_lang: str = None) -> List[str]:\n",
    "        if src_lang is None:\n",
    "            src_langs = [self.detect_language(text) for text in texts]\n",
    "        else:\n",
    "            src_langs = [src_lang] * len(texts)\n",
    "        \n",
    "        translations = []\n",
    "        for text, src_lang in zip(texts, src_langs):\n",
    "            self.tokenizer.src_lang = src_lang\n",
    "            encoded = self.tokenizer(text, return_tensors=\"pt\").to(self.device)\n",
    "            \n",
    "            generated_tokens = self.model.generate(\n",
    "                **encoded,\n",
    "                forced_bos_token_id=self.tokenizer.get_lang_id(tgt_lang),\n",
    "                max_length=128\n",
    "            )\n",
    "            \n",
    "            translations.append(self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0])\n",
    "        \n",
    "        return translations\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     translator = M2M100Translator()\n",
    "\n",
    "#     # Single translation with auto-detection\n",
    "#     text = \"Hello, how are you?\"\n",
    "#     tgt_lang = \"fr\"  # French\n",
    "#     translation = translator.translate(text, tgt_lang)\n",
    "#     print(f\"Original: {text}\")\n",
    "#     print(f\"Translation: {translation}\")\n",
    "\n",
    "#     # Batch translation with auto-detection\n",
    "#     texts = [\"Hello, how are you?\", \"Bonjour, comment allez-vous?\", \"Hola, ¿cómo estás?\"]\n",
    "#     translations = translator.translate_batch(texts, tgt_lang)\n",
    "#     for original, translation in zip(texts, translations):\n",
    "#         print(f\"Original: {original}\")\n",
    "#         print(f\"Translation: {translation}\")\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly! I'll provide you with information about the different model sizes available for the M2M-100 (Many-to-Many 100) model family.\n",
    "\n",
    "M2M-100 comes in three main sizes, each offering a different balance between translation quality and computational requirements. Here are the available variants:\n",
    "\n",
    "1. M2M-100 12B: The largest and most capable model\n",
    "   - Parameters: 12 billion\n",
    "   - Model name: \"facebook/m2m100_12B\"\n",
    "\n",
    "2. M2M-100 1.2B: A medium-sized model\n",
    "   - Parameters: 1.2 billion\n",
    "   - Model name: \"facebook/m2m100_1.2B\"\n",
    "\n",
    "3. M2M-100 418M: The smallest model\n",
    "   - Parameters: 418 million\n",
    "   - Model name: \"facebook/m2m100_418M\"\n",
    "\n",
    "Here's a quick guide on choosing:\n",
    "\n",
    "- For highest quality and if you have powerful hardware: Use the 12B model\n",
    "- For a good balance of quality and speed: Consider the 1.2B model\n",
    "- For faster inference with still reasonable quality: Use the 418M model\n",
    "\n",
    "To use a different model size in the code I provided earlier, you would simply change the model name when initializing the `M2M100Translator` class. For example:\n",
    "\n",
    "```python\n",
    "# For the largest model\n",
    "translator = M2M100Translator(\"facebook/m2m100_12B\")\n",
    "\n",
    "# For the medium-sized model\n",
    "translator = M2M100Translator(\"facebook/m2m100_1.2B\")\n",
    "```\n",
    "\n",
    "It's important to note:\n",
    "\n",
    "1. All M2M-100 models support the same 100 languages, regardless of size.\n",
    "\n",
    "2. Larger models generally provide better translation quality, especially for more complex texts and low-resource language pairs.\n",
    "\n",
    "3. The choice of model size significantly impacts memory usage and inference speed, especially on CPU.\n",
    "\n",
    "4. The 12B model, in particular, may be impractical for CPU-only inference due to its size and computational requirements.\n",
    "\n",
    "For your production use case with CPU inference, the 418M model (as used in the example) might be the most practical choice. If you can afford more memory and longer inference times, the 1.2B model could provide a good balance between quality and performance.\n",
    "\n",
    "Would you like more information on how these different model sizes might affect your implementation or performance? Or would you like to see a comparison of their performance on specific language pairs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
