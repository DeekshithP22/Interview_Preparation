{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18db082d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import TypedDict, Annotated, List, Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_perplexity import ChatPerplexity\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "class SearchDepth(Enum):\n",
    "    BASIC = \"basic\"\n",
    "    ADVANCED = \"advanced\"\n",
    "\n",
    "class SearchProvider(Enum):\n",
    "    PERPLEXITY = \"perplexity\"\n",
    "\n",
    "@dataclass\n",
    "class SearchConfig:\n",
    "    \"\"\"Configuration for search operations\"\"\"\n",
    "    max_results: int = 5\n",
    "    search_depth: SearchDepth = SearchDepth.ADVANCED\n",
    "    include_answer: bool = True\n",
    "    include_raw_content: bool = False\n",
    "    cache_ttl: int = 3600  # 1 hour\n",
    "    timeout: int = 30\n",
    "    max_retries: int = 3\n",
    "    enable_verification: bool = True  # Enable information verification\n",
    "    verification_threshold: float = 0.7  # Minimum consistency score\n",
    "    cross_check_sources: int = 3  # Minimum sources for cross-checking\n",
    "\n",
    "class SearchRequest(BaseModel):\n",
    "    \"\"\"Validated search request\"\"\"\n",
    "    query: str = Field(..., min_length=1, max_length=500)\n",
    "    user_id: Optional[str] = None\n",
    "    session_id: Optional[str] = None\n",
    "    config: Optional[SearchConfig] = None\n",
    "    \n",
    "    @validator('query')\n",
    "    def validate_query(cls, v):\n",
    "        if not v.strip():\n",
    "            raise ValueError(\"Query cannot be empty\")\n",
    "        return v.strip()\n",
    "\n",
    "class VerificationResult(BaseModel):\n",
    "    \"\"\"Information verification results\"\"\"\n",
    "    consistency_score: float = Field(..., ge=0, le=1)\n",
    "    confidence_level: str  # \"high\", \"medium\", \"low\"\n",
    "    conflicting_claims: List[str] = []\n",
    "    supporting_sources: List[str] = []\n",
    "    verification_notes: List[str] = []\n",
    "    fact_checks: Dict[str, Any] = {}\n",
    "\n",
    "class SearchResponse(BaseModel):\n",
    "    \"\"\"Structured search response\"\"\"\n",
    "    query: str\n",
    "    answer: str\n",
    "    perplexity_answer: Optional[str] = None\n",
    "    source_count: int\n",
    "    search_results: Dict[str, Any]\n",
    "    timestamp: datetime\n",
    "    duration_ms: int\n",
    "    cached: bool = False\n",
    "    provider: SearchProvider\n",
    "    session_id: Optional[str] = None\n",
    "    verification: Optional[VerificationResult] = None\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[Any], \"The conversation messages\"]\n",
    "    query: Annotated[str, \"The user's search query\"]\n",
    "    search_results: Annotated[Dict, \"Search results from Perplexity\"]\n",
    "    final_answer: Annotated[str, \"Final answer to the user\"]\n",
    "    config: Annotated[SearchConfig, \"Search configuration\"]\n",
    "    start_time: Annotated[float, \"Request start time\"]\n",
    "    user_id: Annotated[Optional[str], \"User identifier\"]\n",
    "    session_id: Annotated[Optional[str], \"Session identifier\"]\n",
    "    verification_result: Annotated[Optional[VerificationResult], \"Information verification results\"]\n",
    "\n",
    "\n",
    "class PerplexitySearchClient:\n",
    "    \"\"\"Perplexity search client using LangChain ChatPerplexity\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str):\n",
    "        self.api_key = api_key\n",
    "        # Initialize different models for different search depths\n",
    "        self.models = {\n",
    "            \"basic\": ChatPerplexity(\n",
    "                model=\"llama-3.1-sonar-small-128k-online\",\n",
    "                temperature=0.2,\n",
    "                pplx_api_key=api_key\n",
    "            ),\n",
    "            \"advanced\": ChatPerplexity(\n",
    "                model=\"llama-3.1-sonar-large-128k-online\", \n",
    "                temperature=0.2,\n",
    "                pplx_api_key=api_key\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        search_depth: str = \"advanced\",\n",
    "        max_results: int = 5,\n",
    "        include_answer: bool = True,\n",
    "        include_raw_content: bool = False\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform search using Perplexity online models\n",
    "        \n",
    "        Args:\n",
    "            query: Search query\n",
    "            search_depth: Search depth (\"basic\" or \"advanced\")\n",
    "            max_results: Maximum number of results\n",
    "            include_answer: Whether to include AI-generated answer\n",
    "            include_raw_content: Whether to include raw content\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with search results\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Select the appropriate model\n",
    "            model = self.models.get(search_depth, self.models[\"advanced\"])\n",
    "            \n",
    "            # Create enhanced research prompt\n",
    "            research_prompt = f\"\"\"\n",
    "You are a professional research assistant. Please provide a comprehensive answer to this query: {query}\n",
    "\n",
    "Requirements:\n",
    "1. Use current, reliable information from recent sources\n",
    "2. Include specific facts, statistics, and details\n",
    "3. Cite sources when possible\n",
    "4. Structure your response clearly\n",
    "5. If there are multiple perspectives, present them fairly\n",
    "6. Include any relevant recent developments or updates\n",
    "\n",
    "Please provide both a direct answer and supporting information with sources.\n",
    "\"\"\"\n",
    "            \n",
    "            # Configure search parameters using extra_body\n",
    "            extra_params = {\n",
    "                \"search_recency_filter\": \"month\",  # Focus on recent information\n",
    "                \"return_related_questions\": True,\n",
    "                \"return_images\": False\n",
    "            }\n",
    "            \n",
    "            # Invoke the model with search capabilities\n",
    "            response = model.invoke(\n",
    "                research_prompt,\n",
    "                extra_body=extra_params\n",
    "            )\n",
    "            \n",
    "            # Extract the response content\n",
    "            content = response.content\n",
    "            \n",
    "            # Parse response to extract information and simulate sources\n",
    "            sources = self._extract_sources_from_response(content, max_results)\n",
    "            \n",
    "            # Format results to match expected structure\n",
    "            formatted_results = {\n",
    "                \"answer\": content if include_answer else \"\",\n",
    "                \"results\": sources,\n",
    "                \"query\": query,\n",
    "                \"search_metadata\": {\n",
    "                    \"model\": model.model,\n",
    "                    \"search_depth\": search_depth,\n",
    "                    \"timestamp\": datetime.now().isoformat(),\n",
    "                    \"provider\": \"perplexity_langchain\"\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            return formatted_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Perplexity search failed: {str(e)}\")\n",
    "    \n",
    "    def _extract_sources_from_response(self, content: str, max_results: int) -> List[Dict]:\n",
    "        \"\"\"Extract and format sources from Perplexity response\"\"\"\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # Split content into meaningful chunks that could represent different sources\n",
    "        # This is a simplified approach since Perplexity online models aggregate information\n",
    "        paragraphs = [p.strip() for p in content.split('\\n\\n') if p.strip()]\n",
    "        \n",
    "        # Create source entries from content chunks\n",
    "        for i, paragraph in enumerate(paragraphs[:max_results]):\n",
    "            if len(paragraph) > 50:  # Only include substantial paragraphs\n",
    "                result = {\n",
    "                    \"title\": f\"Perplexity Research Finding {i+1}\",\n",
    "                    \"url\": f\"https://perplexity.ai/search?q={'+'.join(str(self.__dict__.get('query', 'research')).split())}\",\n",
    "                    \"content\": paragraph,\n",
    "                    \"score\": 0.9 - (i * 0.1),  # Decreasing relevance score\n",
    "                    \"published_date\": datetime.now().isoformat(),\n",
    "                    \"source\": \"perplexity_online\"\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        # If we don't have enough results from paragraphs, create additional ones\n",
    "        while len(results) < min(max_results, 3):\n",
    "            chunk_size = len(content) // (max_results - len(results))\n",
    "            start_idx = len(results) * chunk_size\n",
    "            end_idx = start_idx + chunk_size\n",
    "            \n",
    "            if start_idx < len(content):\n",
    "                chunk = content[start_idx:end_idx]\n",
    "                if chunk.strip():\n",
    "                    result = {\n",
    "                        \"title\": f\"Perplexity Research Source {len(results)+1}\",\n",
    "                        \"url\": f\"https://perplexity.ai/search/{len(results)+1}\",\n",
    "                        \"content\": chunk.strip(),\n",
    "                        \"score\": 0.9 - (len(results) * 0.1),\n",
    "                        \"published_date\": datetime.now().isoformat(),\n",
    "                        \"source\": \"perplexity_online\"\n",
    "                    }\n",
    "                    results.append(result)\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        return results[:max_results]\n",
    "\n",
    "\n",
    "class InformationVerifier:\n",
    "    \"\"\"Verify information consistency across multiple sources\"\"\"\n",
    "    \n",
    "    def __init__(self, llm):\n",
    "        self.llm = llm\n",
    "    \n",
    "    def verify_information(self, query: str, search_results: Dict, config: SearchConfig) -> VerificationResult:\n",
    "        \"\"\"\n",
    "        Verify information consistency across sources\n",
    "        \n",
    "        Args:\n",
    "            query: Original search query\n",
    "            search_results: Results from search\n",
    "            config: Search configuration\n",
    "            \n",
    "        Returns:\n",
    "            VerificationResult with consistency analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        if not config.enable_verification:\n",
    "            return VerificationResult(\n",
    "                consistency_score=0.5,\n",
    "                confidence_level=\"unknown\",\n",
    "                verification_notes=[\"Verification disabled\"]\n",
    "            )\n",
    "        \n",
    "        results = search_results.get(\"results\", [])\n",
    "        if len(results) < config.cross_check_sources:\n",
    "            return VerificationResult(\n",
    "                consistency_score=0.3,\n",
    "                confidence_level=\"low\",\n",
    "                verification_notes=[f\"Insufficient sources for verification (found {len(results)}, need {config.cross_check_sources})\"]\n",
    "            )\n",
    "        \n",
    "        try:\n",
    "            # Extract key facts from each source\n",
    "            facts_by_source = self._extract_facts_from_sources(query, results)\n",
    "            \n",
    "            # Cross-check facts for consistency\n",
    "            consistency_analysis = self._analyze_consistency(facts_by_source)\n",
    "            \n",
    "            # Evaluate source credibility\n",
    "            credibility_scores = self._evaluate_source_credibility(results)\n",
    "            \n",
    "            # Calculate overall consistency score\n",
    "            overall_score = self._calculate_consistency_score(\n",
    "                consistency_analysis, \n",
    "                credibility_scores\n",
    "            )\n",
    "            \n",
    "            # Determine confidence level\n",
    "            confidence_level = self._determine_confidence_level(overall_score)\n",
    "            \n",
    "            # Identify conflicts\n",
    "            conflicts = self._identify_conflicts(consistency_analysis)\n",
    "            \n",
    "            return VerificationResult(\n",
    "                consistency_score=overall_score,\n",
    "                confidence_level=confidence_level,\n",
    "                conflicting_claims=conflicts,\n",
    "                supporting_sources=[result.get(\"url\", \"\") for result in results[:3]],\n",
    "                verification_notes=self._generate_verification_notes(consistency_analysis, credibility_scores),\n",
    "                fact_checks=consistency_analysis\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Verification failed: {e}\")\n",
    "            return VerificationResult(\n",
    "                consistency_score=0.4,\n",
    "                confidence_level=\"low\",\n",
    "                verification_notes=[f\"Verification failed: {str(e)}\"]\n",
    "            )\n",
    "    \n",
    "    def _extract_facts_from_sources(self, query: str, results: List[Dict]) -> Dict[str, List[str]]:\n",
    "        \"\"\"Extract key facts from each source with robust JSON parsing\"\"\"\n",
    "        \n",
    "        fact_extraction_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a fact extraction expert. Extract key factual claims from the given text related to the query.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Extract only verifiable factual statements\n",
    "2. Ignore opinions, speculations, or subjective claims\n",
    "3. Focus on facts directly related to the query\n",
    "4. Return facts as a JSON list of strings\n",
    "5. Each fact should be concise and specific\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Text: {content}\n",
    "\n",
    "IMPORTANT: Return ONLY a valid JSON array, nothing else. Example:\n",
    "[\"fact1\", \"fact2\", \"fact3\"]\"\"\"),\n",
    "            (\"human\", \"Extract facts from this content\")\n",
    "        ])\n",
    "        \n",
    "        facts_by_source = {}\n",
    "        \n",
    "        for i, result in enumerate(results[:5]):  # Limit to top 5 sources\n",
    "            content = result.get(\"content\", \"\")[:1000]  # Limit content length\n",
    "            if not content.strip():\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                response = self.llm.invoke(\n",
    "                    fact_extraction_prompt.format_messages(\n",
    "                        query=query,\n",
    "                        content=content\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Parse JSON response with robust error handling\n",
    "                facts_text = response.content.strip()\n",
    "                \n",
    "                # Remove markdown code blocks if present\n",
    "                if facts_text.startswith(\"```json\"):\n",
    "                    facts_text = facts_text[7:-3].strip()\n",
    "                elif facts_text.startswith(\"```\"):\n",
    "                    facts_text = facts_text[3:-3].strip()\n",
    "                elif facts_text.startswith(\"`\"):\n",
    "                    facts_text = facts_text[1:-1].strip()\n",
    "                \n",
    "                try:\n",
    "                    facts = json.loads(facts_text)\n",
    "                    # Ensure it's a list and clean the data\n",
    "                    if isinstance(facts, list):\n",
    "                        # Filter out non-string items and empty strings\n",
    "                        clean_facts = [str(fact).strip() for fact in facts if fact and str(fact).strip()]\n",
    "                        facts_by_source[f\"source_{i}\"] = clean_facts\n",
    "                    else:\n",
    "                        print(f\"‚ö† Source {i}: Expected list, got {type(facts)}\")\n",
    "                        facts_by_source[f\"source_{i}\"] = []\n",
    "                        \n",
    "                except json.JSONDecodeError as je:\n",
    "                    print(f\"‚ö† Source {i}: JSON parse error - {je}\")\n",
    "                    print(f\"Raw response: {facts_text[:100]}...\")\n",
    "                    facts_by_source[f\"source_{i}\"] = []\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ö† Source {i}: Fact extraction failed - {e}\")\n",
    "                facts_by_source[f\"source_{i}\"] = []\n",
    "        \n",
    "        # Print extraction summary\n",
    "        total_facts = sum(len(facts) for facts in facts_by_source.values())\n",
    "        print(f\"‚úì Extracted {total_facts} facts from {len(facts_by_source)} sources\")\n",
    "        \n",
    "        return facts_by_source\n",
    "    \n",
    "    def _analyze_consistency(self, facts_by_source: Dict[str, List[str]]) -> Dict[str, Any]:\n",
    "        \"\"\"Analyze consistency across extracted facts with robust parsing\"\"\"\n",
    "        \n",
    "        all_facts = []\n",
    "        for source, facts in facts_by_source.items():\n",
    "            all_facts.extend(facts)\n",
    "        \n",
    "        if not all_facts:\n",
    "            return {\n",
    "                \"consistent_facts\": [], \n",
    "                \"inconsistent_facts\": [], \n",
    "                \"unique_facts\": [],\n",
    "                \"confidence_notes\": [\"No facts extracted\"]\n",
    "            }\n",
    "        \n",
    "        consistency_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a fact-checking expert. Analyze the given facts for consistency.\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Group similar or related facts together\n",
    "2. Identify contradictions or inconsistencies\n",
    "3. Note facts that are consistently reported across sources\n",
    "4. Identify unique facts from single sources\n",
    "\n",
    "Facts to analyze:\n",
    "{facts}\n",
    "\n",
    "IMPORTANT: Return ONLY valid JSON in this exact format:\n",
    "{{\n",
    "    \"consistent_facts\": [\"facts that appear in multiple sources or are compatible\"],\n",
    "    \"inconsistent_facts\": [\"facts that contradict each other\"],\n",
    "    \"unique_facts\": [\"facts from only one source\"],\n",
    "    \"confidence_notes\": [\"brief explanations for consistency assessment\"]\n",
    "}}\"\"\"),\n",
    "            (\"human\", \"Analyze fact consistency\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(\n",
    "                consistency_prompt.format_messages(\n",
    "                    facts=json.dumps(all_facts, indent=2)\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            analysis_text = response.content.strip()\n",
    "            \n",
    "            # Clean JSON response\n",
    "            if analysis_text.startswith(\"```json\"):\n",
    "                analysis_text = analysis_text[7:-3].strip()\n",
    "            elif analysis_text.startswith(\"```\"):\n",
    "                analysis_text = analysis_text[3:-3].strip()\n",
    "            \n",
    "            try:\n",
    "                analysis = json.loads(analysis_text)\n",
    "                \n",
    "                # Validate and ensure all required keys exist\n",
    "                required_keys = [\"consistent_facts\", \"inconsistent_facts\", \"unique_facts\", \"confidence_notes\"]\n",
    "                for key in required_keys:\n",
    "                    if key not in analysis or not isinstance(analysis[key], list):\n",
    "                        analysis[key] = []\n",
    "                \n",
    "                print(f\"‚úì Consistency analysis: {len(analysis['consistent_facts'])} consistent, {len(analysis['inconsistent_facts'])} inconsistent\")\n",
    "                return analysis\n",
    "                \n",
    "            except json.JSONDecodeError as je:\n",
    "                print(f\"‚ö† Consistency analysis JSON parse error: {je}\")\n",
    "                print(f\"Raw response: {analysis_text[:200]}...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö† Consistency analysis failed: {e}\")\n",
    "        \n",
    "        # Fallback response\n",
    "        return {\n",
    "            \"consistent_facts\": all_facts[:3],  # Assume first few are consistent\n",
    "            \"inconsistent_facts\": [],\n",
    "            \"unique_facts\": all_facts[3:],\n",
    "            \"confidence_notes\": [\"Analysis failed - using fallback\"]\n",
    "        }\n",
    "    \n",
    "    def _evaluate_source_credibility(self, results: List[Dict]) -> Dict[str, float]:\n",
    "        \"\"\"Evaluate credibility of sources\"\"\"\n",
    "        credibility_scores = {}\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            score = 0.5  # Base score\n",
    "            url = result.get(\"url\", \"\").lower()\n",
    "            title = result.get(\"title\", \"\").lower()\n",
    "            \n",
    "            # Domain-based scoring\n",
    "            if any(domain in url for domain in [\".edu\", \".gov\", \".org\"]):\n",
    "                score += 0.3\n",
    "            elif any(domain in url for domain in [\".com\", \".net\"]):\n",
    "                score += 0.1\n",
    "            \n",
    "            # Known credible sources\n",
    "            credible_domains = [\n",
    "                \"wikipedia.org\", \"britannica.com\", \"nature.com\", \n",
    "                \"sciencedirect.com\", \"pubmed.ncbi.nlm.nih.gov\",\n",
    "                \"who.int\", \"cdc.gov\", \"fda.gov\", \"nih.gov\",\n",
    "                \"reuters.com\", \"bbc.com\", \"apnews.com\"\n",
    "            ]\n",
    "            \n",
    "            if any(domain in url for domain in credible_domains):\n",
    "                score += 0.2\n",
    "            \n",
    "            # Content quality indicators\n",
    "            content_length = len(result.get(\"content\", \"\"))\n",
    "            if content_length > 500:\n",
    "                score += 0.1\n",
    "            \n",
    "            # Perplexity relevance score\n",
    "            perplexity_score = result.get(\"score\", 0)\n",
    "            score += min(perplexity_score * 0.2, 0.2)\n",
    "            \n",
    "            # Ensure score is between 0 and 1\n",
    "            credibility_scores[f\"source_{i}\"] = min(max(score, 0.0), 1.0)\n",
    "        \n",
    "        return credibility_scores\n",
    "    \n",
    "    def _calculate_consistency_score(\n",
    "        self, \n",
    "        consistency_analysis: Dict[str, Any], \n",
    "        credibility_scores: Dict[str, float]\n",
    "    ) -> float:\n",
    "        \"\"\"Calculate overall consistency score\"\"\"\n",
    "        \n",
    "        consistent_facts = len(consistency_analysis.get(\"consistent_facts\", []))\n",
    "        inconsistent_facts = len(consistency_analysis.get(\"inconsistent_facts\", []))\n",
    "        total_facts = consistent_facts + inconsistent_facts + len(consistency_analysis.get(\"unique_facts\", []))\n",
    "        \n",
    "        if total_facts == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        # Base consistency ratio\n",
    "        if consistent_facts + inconsistent_facts == 0:\n",
    "            consistency_ratio = 0.5\n",
    "        else:\n",
    "            consistency_ratio = consistent_facts / (consistent_facts + inconsistent_facts)\n",
    "        \n",
    "        # Weight by source credibility\n",
    "        avg_credibility = sum(credibility_scores.values()) / len(credibility_scores) if credibility_scores else 0.5\n",
    "        \n",
    "        # Penalize for conflicts\n",
    "        conflict_penalty = min(inconsistent_facts * 0.1, 0.3)\n",
    "        \n",
    "        # Final score\n",
    "        final_score = (consistency_ratio * 0.6 + avg_credibility * 0.4) - conflict_penalty\n",
    "        \n",
    "        return min(max(final_score, 0.0), 1.0)\n",
    "    \n",
    "    def _determine_confidence_level(self, score: float) -> str:\n",
    "        \"\"\"Determine confidence level based on score\"\"\"\n",
    "        if score >= 0.8:\n",
    "            return \"high\"\n",
    "        elif score >= 0.6:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def _identify_conflicts(self, consistency_analysis: Dict[str, Any]) -> List[str]:\n",
    "        \"\"\"Identify specific conflicting claims\"\"\"\n",
    "        return consistency_analysis.get(\"inconsistent_facts\", [])\n",
    "    \n",
    "    def _generate_verification_notes(\n",
    "        self, \n",
    "        consistency_analysis: Dict[str, Any], \n",
    "        credibility_scores: Dict[str, float]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Generate human-readable verification notes\"\"\"\n",
    "        notes = []\n",
    "        \n",
    "        consistent_count = len(consistency_analysis.get(\"consistent_facts\", []))\n",
    "        inconsistent_count = len(consistency_analysis.get(\"inconsistent_facts\", []))\n",
    "        \n",
    "        notes.append(f\"Found {consistent_count} consistent facts across sources\")\n",
    "        \n",
    "        if inconsistent_count > 0:\n",
    "            notes.append(f\"Detected {inconsistent_count} potential contradictions\")\n",
    "        \n",
    "        avg_credibility = sum(credibility_scores.values()) / len(credibility_scores) if credibility_scores else 0\n",
    "        notes.append(f\"Average source credibility: {avg_credibility:.2f}\")\n",
    "        \n",
    "        if any(score > 0.8 for score in credibility_scores.values()):\n",
    "            notes.append(\"High-credibility sources found\")\n",
    "        \n",
    "        return notes\n",
    "\n",
    "\n",
    "class SearchAgent:\n",
    "    \"\"\"Search agent with information verification using Perplexity\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        google_api_key: Optional[str] = None,\n",
    "        perplexity_api_key: Optional[str] = None\n",
    "    ):\n",
    "        # Validate API keys\n",
    "        self.google_api_key = google_api_key or os.getenv(\"GEMINI_API_KEY\")\n",
    "        self.perplexity_api_key = perplexity_api_key or os.getenv(\"PPLX_API_KEY\")\n",
    "        \n",
    "        if not self.google_api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY environment variable is required\")\n",
    "        if not self.perplexity_api_key:\n",
    "            raise ValueError(\"PPLX_API_KEY environment variable is required\")\n",
    "        \n",
    "        # Initialize components\n",
    "        try:\n",
    "            print(\"üöÄ Initializing SearchAgent...\")\n",
    "            \n",
    "            self.llm = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-pro\",\n",
    "                temperature=0.1,\n",
    "                google_api_key=self.google_api_key\n",
    "            )\n",
    "            \n",
    "            self.perplexity_client = PerplexitySearchClient(api_key=self.perplexity_api_key)\n",
    "            self.verifier = InformationVerifier(self.llm)\n",
    "            \n",
    "            print(\"‚úì API keys validated\")\n",
    "            print(\"‚úì LLM and search client initialized\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Failed to initialize APIs: {str(e)}\")\n",
    "        \n",
    "        # Create the graph\n",
    "        self.graph = self._create_graph()\n",
    "        print(\"‚úì SearchAgent ready!\")\n",
    "    \n",
    "    def _search_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Execute Perplexity search\"\"\"\n",
    "        query = state[\"query\"]\n",
    "        config = state[\"config\"]\n",
    "        \n",
    "        try:\n",
    "            print(f\"üîç Searching: {query}\")\n",
    "            \n",
    "            search_results = self.perplexity_client.search(\n",
    "                query=query,\n",
    "                search_depth=config.search_depth.value,\n",
    "                max_results=config.max_results,\n",
    "                include_answer=config.include_answer,\n",
    "                include_raw_content=config.include_raw_content\n",
    "            )\n",
    "            \n",
    "            state[\"search_results\"] = search_results\n",
    "            source_count = len(search_results.get(\"results\", []))\n",
    "            print(f\"‚úì Search completed: {source_count} sources found\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Search failed: {e}\")\n",
    "            state[\"search_results\"] = {\n",
    "                \"error\": str(e),\n",
    "                \"results\": [],\n",
    "                \"answer\": \"\"\n",
    "            }\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _verify_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Verify information consistency across sources\"\"\"\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        search_results = state[\"search_results\"]\n",
    "        config = state[\"config\"]\n",
    "        \n",
    "        if not config.enable_verification:\n",
    "            print(\"‚è≠ Verification disabled\")\n",
    "            state[\"verification_result\"] = VerificationResult(\n",
    "                consistency_score=0.5,\n",
    "                confidence_level=\"unknown\",\n",
    "                verification_notes=[\"Verification disabled\"]\n",
    "            )\n",
    "            return state\n",
    "        \n",
    "        print(\"üîç Verifying information consistency...\")\n",
    "        \n",
    "        try:\n",
    "            verification_result = self.verifier.verify_information(\n",
    "                query, search_results, config\n",
    "            )\n",
    "            \n",
    "            state[\"verification_result\"] = verification_result\n",
    "            \n",
    "            print(f\"‚úì Verification completed: {verification_result.confidence_level} confidence ({verification_result.consistency_score:.2f})\")\n",
    "            \n",
    "            if verification_result.conflicting_claims:\n",
    "                print(f\"‚ö† {len(verification_result.conflicting_claims)} conflicts detected\")\n",
    "                for i, conflict in enumerate(verification_result.conflicting_claims[:2], 1):\n",
    "                    print(f\"  {i}. {conflict}\")\n",
    "            else:\n",
    "                print(\"‚úì No major conflicts detected\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Verification failed: {e}\")\n",
    "            state[\"verification_result\"] = VerificationResult(\n",
    "                consistency_score=0.5,\n",
    "                confidence_level=\"unknown\",\n",
    "                verification_notes=[f\"Verification failed: {e}\"]\n",
    "            )\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _answer_node(self, state: AgentState) -> AgentState:\n",
    "        \"\"\"Generate answer with verification context\"\"\"\n",
    "        \n",
    "        query = state[\"query\"]\n",
    "        search_results = state[\"search_results\"]\n",
    "        verification_result = state.get(\"verification_result\")\n",
    "        \n",
    "        print(\"ü§ñ Generating answer with verification context...\")\n",
    "        \n",
    "        # Format search results\n",
    "        if \"error\" in search_results:\n",
    "            results_text = f\"Search Error: {search_results['error']}\"\n",
    "        else:\n",
    "            results_text = self._format_search_results(search_results)\n",
    "        \n",
    "        # Add verification context\n",
    "        verification_context = \"\"\n",
    "        if verification_result:\n",
    "            verification_context = f\"\"\"\n",
    "VERIFICATION ANALYSIS:\n",
    "- Consistency Score: {verification_result.consistency_score:.2f}\n",
    "- Confidence Level: {verification_result.confidence_level}\n",
    "- Supporting Sources: {len(verification_result.supporting_sources)}\n",
    "- Conflicting Claims: {len(verification_result.conflicting_claims)}\n",
    "\n",
    "{f\"CONFLICTS DETECTED: {verification_result.conflicting_claims}\" if verification_result.conflicting_claims else \"\"}\n",
    "\n",
    "VERIFICATION NOTES:\n",
    "{chr(10).join(verification_result.verification_notes)}\n",
    "\"\"\"\n",
    "        \n",
    "        # Create enhanced prompt with verification\n",
    "        answer_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"You are a professional AI research assistant with fact-checking capabilities. Provide accurate, well-structured answers based on search results and verification analysis.\n",
    "\n",
    "CRITICAL GUIDELINES:\n",
    "1. Use search results as primary information source\n",
    "2. ALWAYS consider the verification analysis when forming your answer\n",
    "3. If consistency score is LOW (< 0.6), mention uncertainty and conflicting information\n",
    "4. If conflicts are detected, acknowledge them explicitly\n",
    "5. Cite sources with URLs when possible\n",
    "6. Structure answers with clear sections\n",
    "7. Provide confidence indicators based on verification\n",
    "\n",
    "SEARCH RESULTS:\n",
    "{search_results}\n",
    "\n",
    "{verification_context}\n",
    "\n",
    "RESPONSE FORMAT:\n",
    "- Start with a direct answer to the question\n",
    "- Include confidence level based on verification\n",
    "- Mention any limitations or conflicts\n",
    "- Provide supporting details with source citations\n",
    "- End with additional context if relevant\"\"\"),\n",
    "            (\"human\", \"Question: {query}\")\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(\n",
    "                answer_prompt.format_messages(\n",
    "                    query=query,\n",
    "                    search_results=results_text,\n",
    "                    verification_context=verification_context\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            state[\"final_answer\"] = response.content\n",
    "            state[\"messages\"].append(AIMessage(content=response.content))\n",
    "            \n",
    "            print(\"‚úì Answer generated with verification context\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"Answer generation failed: {str(e)}\"\n",
    "            state[\"final_answer\"] = error_msg\n",
    "            state[\"messages\"].append(AIMessage(content=error_msg))\n",
    "            print(f\"‚úó Answer generation failed: {e}\")\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def _format_search_results(self, search_results: Dict) -> str:\n",
    "        \"\"\"Format search results for LLM consumption\"\"\"\n",
    "        results_text = \"\"\n",
    "        \n",
    "        # Add Perplexity AI answer\n",
    "        if search_results.get(\"answer\"):\n",
    "            results_text += f\"AI SUMMARY: {search_results['answer']}\\n\\n\"\n",
    "        \n",
    "        # Add search results\n",
    "        if search_results.get(\"results\"):\n",
    "            results_text += \"SOURCES:\\n\"\n",
    "            for i, result in enumerate(search_results[\"results\"], 1):\n",
    "                results_text += f\"{i}. {result.get('title', 'Untitled')}\\n\"\n",
    "                results_text += f\"   URL: {result.get('url', 'No URL')}\\n\"\n",
    "                results_text += f\"   Content: {result.get('content', 'No content')[:400]}...\\n\"\n",
    "                results_text += f\"   Relevance Score: {result.get('score', 'N/A')}\\n\\n\"\n",
    "        \n",
    "        return results_text\n",
    "    \n",
    "    def _create_graph(self) -> StateGraph:\n",
    "        \"\"\"Create production workflow with verification\"\"\"\n",
    "        workflow = StateGraph(AgentState)\n",
    "        \n",
    "        workflow.add_node(\"search\", self._search_node)\n",
    "        workflow.add_node(\"verify\", self._verify_node)\n",
    "        workflow.add_node(\"answer\", self._answer_node)\n",
    "        \n",
    "        workflow.set_entry_point(\"search\")\n",
    "        workflow.add_edge(\"search\", \"verify\")\n",
    "        workflow.add_edge(\"verify\", \"answer\")\n",
    "        workflow.add_edge(\"answer\", END)\n",
    "        \n",
    "        return workflow.compile()\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        request: SearchRequest,\n",
    "        config: Optional[SearchConfig] = None\n",
    "    ) -> SearchResponse:\n",
    "        \"\"\"Main search method with full observability\"\"\"\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Use provided config or default\n",
    "        search_config = config or request.config or SearchConfig()\n",
    "        \n",
    "        print(f\"üìä Verification: {'enabled' if search_config.enable_verification else 'disabled'}\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare initial state\n",
    "            initial_state = {\n",
    "                \"messages\": [HumanMessage(content=request.query)],\n",
    "                \"query\": request.query,\n",
    "                \"search_results\": {},\n",
    "                \"final_answer\": \"\",\n",
    "                \"config\": search_config,\n",
    "                \"start_time\": start_time,\n",
    "                \"user_id\": request.user_id,\n",
    "                \"session_id\": request.session_id,\n",
    "                \"verification_result\": None\n",
    "            }\n",
    "            \n",
    "            # Execute workflow\n",
    "            final_state = self.graph.invoke(initial_state)\n",
    "            \n",
    "            # Calculate duration\n",
    "            duration_ms = int((time.time() - start_time) * 1000)\n",
    "            \n",
    "            # Create response\n",
    "            response = SearchResponse(\n",
    "                query=request.query,\n",
    "                answer=final_state.get(\"final_answer\", \"\"),\n",
    "                perplexity_answer=final_state.get(\"search_results\", {}).get(\"answer\"),\n",
    "                source_count=len(final_state.get(\"search_results\", {}).get(\"results\", [])),\n",
    "                search_results=final_state.get(\"search_results\", {}),\n",
    "                timestamp=datetime.now(),\n",
    "                duration_ms=duration_ms,\n",
    "                cached=final_state.get(\"search_results\", {}).get(\"cached\", False),\n",
    "                provider=SearchProvider.PERPLEXITY,\n",
    "                session_id=request.session_id,\n",
    "                verification=final_state.get(\"verification_result\")\n",
    "            )\n",
    "            \n",
    "            print(f\"‚è± Completed in {duration_ms}ms\")\n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Search request failed: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "# Convenience function for simple usage\n",
    "def simple_search(query: str, enable_verification: bool = True) -> SearchResponse:\n",
    "    \"\"\"Simple search function for quick usage\"\"\"\n",
    "    agent = SearchAgent()\n",
    "    request = SearchRequest(query=query)\n",
    "    config = SearchConfig(enable_verification=enable_verification)\n",
    "    return agent.search(request, config)\n",
    "\n",
    "\n",
    "# Example usage with comprehensive testing\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Initialize agent\n",
    "        agent = SearchAgent()\n",
    "        \n",
    "        # Test searches with different configurations\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"query\": \"What are the health benefits of intermittent fasting?\",\n",
    "                \"config\": SearchConfig(enable_verification=True, max_results=5)\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Latest research on COVID-19 vaccine effectiveness\",\n",
    "                \"config\": SearchConfig(enable_verification=True, max_results=7, cross_check_sources=4)\n",
    "            },\n",
    "            {\n",
    "                \"query\": \"Dr. Anthony Fauci research background and publications\",\n",
    "                \"config\": SearchConfig(enable_verification=True, search_depth=SearchDepth.ADVANCED)\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"üß™ TEST {i}/{len(test_cases)}\")\n",
    "            print('='*80)\n",
    "            \n",
    "            try:\n",
    "                # Create search request\n",
    "                request = SearchRequest(\n",
    "                    query=test_case[\"query\"],\n",
    "                    user_id=f\"test_user_{i}\",\n",
    "                    session_id=f\"session_{i}\"\n",
    "                )\n",
    "                \n",
    "                # Execute search\n",
    "                result = agent.search(request, test_case[\"config\"])\n",
    "                \n",
    "                print(f\"\\nüìã RESULTS SUMMARY:\")\n",
    "                print(f\"Query: {result.query}\")\n",
    "                print(f\"Sources: {result.source_count}\")\n",
    "                print(f\"Duration: {result.duration_ms}ms\")\n",
    "                print(f\"Provider: {result.provider.value}\")\n",
    "                \n",
    "                if result.verification:\n",
    "                    print(f\"Verification: {result.verification.confidence_level} confidence ({result.verification.consistency_score:.2f})\")\n",
    "                    if result.verification.conflicting_claims:\n",
    "                        print(f\"‚ö† Conflicts: {len(result.verification.conflicting_claims)} detected\")\n",
    "                        for j, conflict in enumerate(result.verification.conflicting_claims[:2], 1):\n",
    "                            print(f\"  {j}. {conflict}\")\n",
    "                    else:\n",
    "                        print(\"‚úì No conflicts detected\")\n",
    "                \n",
    "                print(f\"\\nüí¨ ANSWER PREVIEW:\")\n",
    "                answer_preview = result.answer[:300] + \"...\" if len(result.answer) > 300 else result.answer\n",
    "                print(answer_preview)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Test {i} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"\\nüéâ All tests completed!\")\n",
    "        \n",
    "        # Example of simple usage\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"üìù SIMPLE USAGE EXAMPLE\")\n",
    "        print('='*80)\n",
    "        simple_result = simple_search(\"What are symptoms of diabetes?\")\n",
    "        print(f\"Simple search result: {simple_result.answer[:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to initialize SearchAgent: {e}\")\n",
    "        print(\"\\nüí° Make sure you have set these environment variables:\")\n",
    "        print(\"   export GEMINI_API_KEY='your_google_api_key'\")\n",
    "        print(\"   export PPLX_API_KEY='your_perplexity_api_key'\")\n",
    "        print(\"\\nüì¶ Required dependencies:\")\n",
    "        print(\"   pip install langgraph langchain-google-genai langchain-perplexity pydantic\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".cvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
