{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextTranslationService:\n",
    "    def __init__(self, azure_openai_key: str, azure_openai_endpoint: str,\n",
    "                 text_analytics_key: str, text_analytics_endpoint: str,\n",
    "                 translator_key: str, translator_endpoint: str):\n",
    "        self.openai.api_type = \"azure\"\n",
    "        self.openai.api_base = azure_openai_endpoint\n",
    "        self.openai.api_version = \"2023-05-15\"\n",
    "        self.openai.api_key = azure_openai_key\n",
    "\n",
    "        # self.text_analytics_client = TextAnalyticsClient(\n",
    "        #     endpoint=text_analytics_endpoint,\n",
    "        #     credential=AzureKeyCredential(text_analytics_key)\n",
    "        # )\n",
    "\n",
    "        # self.translator_client = TranslatorTextClient(\n",
    "        #     credentials=ApiKeyCredentials({\"Ocp-Apim-Subscription-Key\": translator_key}),\n",
    "        #     endpoint=translator_endpoint\n",
    "        # )\n",
    "\n",
    "    def identify_input_type(self, content: str) -> str:\n",
    "        prompt = f\"Identify the type of the following content. Possible types are HTML, XML, JSON, or plain text. Content: {content[:500]}\"\n",
    "        response = openai.Completion.create(engine=\"YOUR_ENGINE_NAME\", prompt=prompt, max_tokens=50)\n",
    "        return response.choices[0].text.strip().lower()\n",
    "\n",
    "    def extract_translatable_content(self, content: str, input_type: str) -> List[str]:\n",
    "        prompt = f\"Extract the translatable content from the following {input_type} input. Return the content as a Python list of strings. Input: {content}\"\n",
    "        response = openai.Completion.create(engine=\"YOUR_ENGINE_NAME\", prompt=prompt, max_tokens=1000)\n",
    "        return eval(response.choices[0].text.strip())\n",
    "\n",
    "    def identify_domain_specific_words(self, content: List[str]) -> List[str]:\n",
    "        documents = [\" \".join(content)]\n",
    "        response = self.text_analytics_client.recognize_entities(documents)\n",
    "        domain_specific_words = []\n",
    "        for result in response:\n",
    "            if result.is_error:\n",
    "                continue\n",
    "            for entity in result.entities:\n",
    "                if entity.category in [\"Organization\", \"Person\", \"Location\", \"Product\"]:\n",
    "                    domain_specific_words.append(entity.text)\n",
    "        return list(set(domain_specific_words))\n",
    "\n",
    "    def translate_content(self, content: List[str], target_language: str, domain_specific_words: List[str]) -> List[str]:\n",
    "        translated_content = []\n",
    "        for text in content:\n",
    "            for word in domain_specific_words:\n",
    "                text = text.replace(word, f\"<keep>{word}</keep>\")\n",
    "            \n",
    "            params = TranslateRequestParams(text=text, to=[target_language])\n",
    "            response = self.translator_client.translate(params)\n",
    "            \n",
    "            translated_text = response[0].translations[0].text\n",
    "            for word in domain_specific_words:\n",
    "                translated_text = translated_text.replace(f\"<keep>{word}</keep>\", word)\n",
    "            \n",
    "            translated_content.append(translated_text)\n",
    "        \n",
    "        return translated_content\n",
    "\n",
    "    def reconstruct_output(self, original_content: str, translated_content: List[str], input_type: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Reconstruct the original {input_type} format using the translated content.\n",
    "        Original content structure: {original_content}\n",
    "        Translated content: {translated_content}\n",
    "        Return the reconstructed {input_type} content.\n",
    "        \"\"\"\n",
    "        response = openai.Completion.create(engine=\"YOUR_ENGINE_NAME\", prompt=prompt, max_tokens=1000)\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "    def process_file(self, file_content: str, target_language: str) -> str:\n",
    "        input_type = self.identify_input_type(file_content)\n",
    "        translatable_content = self.extract_translatable_content(file_content, input_type)\n",
    "        domain_specific_words = self.identify_domain_specific_words(translatable_content)\n",
    "        translated_content = self.translate_content(translatable_content, target_language, domain_specific_words)\n",
    "        reconstructed_output = self.reconstruct_output(file_content, translated_content, input_type)\n",
    "        return reconstructed_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from typing import Dict, List, Union\n",
    "from bs4 import BeautifulSoup\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "# from azure.ai.textanalytics import TextAnalyticsClient\n",
    "# from azure.cognitiveservices.language.translatortext import TranslatorTextClient\n",
    "# from azure.cognitiveservices.language.translatortext.models import TranslateRequestParams\n",
    "# from msrest.authentication import ApiKeyCredentials\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage for different file types\n",
    "html_content = \"<html><body><h1>Hello, World!</h1><p>This is a test.</p></body></html>\"\n",
    "xml_content = \"<root><item>Translate me</item><item>And me too</item></root>\"\n",
    "json_content = '{\"title\": \"Translate this\", \"description\": \"And this as well\"}'\n",
    "plain_text = \"This is a plain text file that needs translation.\"\n",
    "\n",
    "target_language = \"es\"  # Spanish\n",
    "\n",
    "for content in [html_content, xml_content, json_content, plain_text]:\n",
    "    translated_output = translation_service.process_file(content, target_language)\n",
    "    print(f\"Translated output: {translated_output}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from typing import Dict, List, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "class LLMTextProcessingService:\n",
    "    def __init__(self, openai_key: str, openai_org:str, openai_proj:str):\n",
    "        openai.api_key = openai_key\n",
    "        openai.organization = openai_org\n",
    "        openai.project = openai_proj\n",
    "\n",
    "    def _query_llm(self, prompt: str) -> str:\n",
    "        # response = openai.chat(\n",
    "        #     engine=\"gpt-4o-2024-08-06\",\n",
    "        #     prompt=prompt,\n",
    "        # )\n",
    "        client = OpenAI(\n",
    "            api_key = \"\",\n",
    "            organization= \"\",\n",
    "            project= \"\",\n",
    "        )\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-2024-08-06\",  # Use \"gpt-4\" or \"gpt-3.5-turbo\"\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=50\n",
    "        )\n",
    "        print(response.choices[0].text.strip())\n",
    "    \n",
    "    def identify_input_type(self, content: str) -> str:\n",
    "        prompt = f\"\"\"Identify the type of the following content. \n",
    "        Possible types are HTML, XML, JSON, or plain text. \n",
    "        Respond with only the type in lowercase.\n",
    "        Content: {content[:500]}\"\"\"\n",
    "        \n",
    "        return self._query_llm(prompt)\n",
    "\n",
    "    def extract_translatable_content(self, content: str, input_type: str) -> List[str]:\n",
    "        prompt = f\"\"\"Extract the translatable content from the following {input_type} input.\n",
    "        Return the content as a Python list of strings, where each string is a separate piece of text to be translated.\n",
    "        Do not include any markup or structural elements, only the text content.\n",
    "        Input: {content}\"\"\"\n",
    "        \n",
    "        result = self._query_llm(prompt)\n",
    "        return eval(result)  # Convert the string representation of a list to an actual list\n",
    "\n",
    "    # def identify_domain_specific_words(self, content: List[str]) -> List[str]:\n",
    "    #     prompt = f\"\"\"Identify domain-specific words or phrases from the following content.\n",
    "    #     These are words that should not be translated, such as proper nouns, technical terms, or brand names.\n",
    "    #     Return the identified words as a Python list of strings.\n",
    "    #     Content: {' '.join(content[:100])}\"\"\"  # Limiting to first 100 items for prompt length\n",
    "        \n",
    "    #     result = self._query_llm(prompt)\n",
    "    #     return eval(result)  # Convert the string representation of a list to an actual list\n",
    "\n",
    "    def reconstruct_output(self, original_content: str, translated_content: List[str], input_type: str) -> str:\n",
    "        prompt = f\"\"\"Reconstruct the original {input_type} format using the translated content.\n",
    "        Original content structure: {original_content}\n",
    "        Translated content: {translated_content}\n",
    "        Ensure that the structure and formatting of the original content is preserved,\n",
    "        replacing only the translatable text with the corresponding translated content.\n",
    "        Return the reconstructed {input_type} content.\"\"\"\n",
    "        \n",
    "        return self._query_llm(prompt, max_tokens=2000)\n",
    "\n",
    "    def process_file(self, file_content: str) -> Dict[str, Union[str, List[str]]]:\n",
    "        input_type = self.identify_input_type(file_content)\n",
    "        translatable_content = self.extract_translatable_content(file_content, input_type)\n",
    "        domain_specific_words = self.identify_domain_specific_words(translatable_content)\n",
    "        \n",
    "        return {\n",
    "            \"input_type\": input_type,\n",
    "            \"translatable_content\": translatable_content,\n",
    "            \"domain_specific_words\": domain_specific_words,\n",
    "            \"original_content\": file_content\n",
    "        }\n",
    "\n",
    "    def reconstruct_file(self, processed_data: Dict[str, Union[str, List[str]]], translated_content: List[str]) -> str:\n",
    "        return self.reconstruct_output(\n",
    "            processed_data[\"original_content\"],\n",
    "            translated_content,\n",
    "            processed_data[\"input_type\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    openai_key = \"\"\n",
    "    openai_org = \"\"\n",
    "    openai_proj = \"\"\n",
    "\n",
    "    llm_text_processing_service = LLMTextProcessingService(openai_key, openai_org, openai_proj)\n",
    "\n",
    "    # Example usage for different file types\n",
    "    html_content = \"<html><body><h1>Hello, World!</h1><p>This is a test.</p></body></html>\"\n",
    "    xml_content = \"<root><item>Translate me</item><item>And me too</item></root>\"\n",
    "    json_content = '{\"title\": \"Translate this\", \"description\": \"And this as well\"}'\n",
    "    plain_text = \"This is a plain text file that needs translation.\"\n",
    "\n",
    "    for content in [html_content, xml_content, json_content, plain_text]:\n",
    "        processed_data = llm_text_processing_service.process_file(content)\n",
    "        print(f\"Input Type: {processed_data['input_type']}\")\n",
    "        print(f\"Translatable Content: {processed_data['translatable_content']}\")\n",
    "        print(f\"Domain-specific Words: {processed_data['domain_specific_words']}\")\n",
    "        print(\"---\")\n",
    "\n",
    "        # Simulating translation (replace this with your actual translation API call)\n",
    "        translated_content = [f\"Translated: {text}\" for text in processed_data['translatable_content']]\n",
    "\n",
    "        reconstructed_content = llm_text_processing_service.reconstruct_file(processed_data, translated_content)\n",
    "        print(f\"Reconstructed Content: {reconstructed_content}\")\n",
    "        print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI \n",
    "import os\n",
    "\n",
    "## Set the API key and model name\n",
    "MODEL=\"gpt-4o-2024-08-06\"\n",
    "client = OpenAI(\n",
    "    api_key=\"\",\n",
    "    organization=\"\",\n",
    "    project=\"\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "  model=MODEL,\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Help me with my math homework!\"}, # <-- This is the system message that provides context to the model\n",
    "    {\"role\": \"user\", \"content\": \"Hello! Could you solve 2+2?\"}  # <-- This is the user message for which the model will generate a response\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"Assistant: \" + completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_translatable_content(self, content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "    prompt = f\"\"\"\n",
    "        Analyze and extract the translatable content from the following {input_type} input.\n",
    "        Return the content as a Python list of dictionaries, where each dictionary represents a translatable element with the \n",
    "        following structure:\n",
    "        {[{\n",
    "            \"type\": \"The type of element (e.g., 'paragraph', 'heading', 'list_item', 'table_cell', etc.)\",\n",
    "            \"content\": \"The actual text content to be translated\",\n",
    "            \"metadata\": {{\n",
    "                \"tag\": \"The original HTML tag or markdown syntax (if applicable)\",\n",
    "                \"attributes\": \"Any relevant attributes (for HTML)\",\n",
    "                \"formatting\": \"Any special formatting instructions\"\n",
    "            }},\n",
    "            \"non_translatable\": [\"List of domain-specific terms or phrases that should not be translated\"]\n",
    "        }]}\n",
    "\n",
    "        Rules:\n",
    "        1. Preserve the structure of the original {input_type} input.\n",
    "        2. Identify and mark domain-specific terms or technical jargon as non-translatable.\n",
    "        3. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "        4. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "        Input: {content}\n",
    "\n",
    "        Ensure the output is a valid Python list of dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    result = self._query_llm(prompt)\n",
    "    return eval(result)  # Convert the string representation of a list of dictionaries to the actual data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_output(self, original_content: str, extracted_structure: List[Dict[str, Union[str, List[str]]]], translated_content: List[str], input_type: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        Reconstruct the original {input_type} format using the translated content and extracted structure.\n",
    "\n",
    "        Original content: \n",
    "        {original_content}\n",
    "\n",
    "        Extracted structure:\n",
    "        {extracted_structure}\n",
    "\n",
    "        Translated content (in order of extraction):\n",
    "        {translated_content}\n",
    "\n",
    "        Instructions:\n",
    "        1. Use the extracted structure to guide the reconstruction process.\n",
    "        2. Replace the original text in each extracted element with the corresponding translated text.\n",
    "        3. Preserve all original formatting, tags, attributes, and non-translatable content.\n",
    "        4. Ensure that the reconstructed content maintains the same structure and order as the original.\n",
    "        5. For any domain-specific terms or proper nouns that were marked as non-translatable, use the original text.\n",
    "        6. If there are any placeholders or variables in the original content, ensure they are correctly maintained in the translated version.\n",
    "\n",
    "        Return the fully reconstructed {input_type} content, ensuring it's a valid and well-formatted {input_type} document.\n",
    "    \"\"\"\n",
    "\n",
    "    result = self._query_llm(prompt)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_input_type(self, content: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "        Analyze the following content and identify its type. Respond with a single word in lowercase, choosing from:\n",
    "            - html\n",
    "            - xml\n",
    "            - json\n",
    "            - markdown\n",
    "            - yaml\n",
    "            - csv\n",
    "            - plaintext\n",
    "\n",
    "        Use 'plaintext' if the content doesn't match any specific format.\n",
    "        \n",
    "        Guidelines:\n",
    "        - Look for distinctive markers like HTML tags, XML declarations, JSON brackets, or Markdown syntax.\n",
    "        - Consider structure and formatting, not just the presence of certain characters.\n",
    "        - If multiple formats are present, choose the predominant one.\n",
    "\n",
    "        Content: {content[:1000]}\n",
    "\n",
    "        Type:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return self._query_llm(prompt).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_content_for_translation(data):\n",
    "    return [item[\"content\"] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming evaluated_result contains the list of dictionaries\n",
    "translated_content = [element[\"content\"] for element in evaluated_result]\n",
    "print(translated_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_content(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have an instance of the class that contains identify_input_type\n",
    "# analyzer = YourClass()\n",
    "\n",
    "# Read the contents of your files\n",
    "html_content = read_file_content(r'C:\\Users\\asua\\DataScience Exercises\\Desktop\\index.html')\n",
    "# xml_content = read_file_content('path/to/your/xml_file.xml')\n",
    "# json_content = read_file_content('path/to/your/json_file.json')\n",
    "\n",
    "# Now you can call identify_input_type with the file contents\n",
    "# html_type = analyzer.identify_input_type(html_content)\n",
    "# xml_type = analyzer.identify_input_type(xml_content)\n",
    "# json_type = analyzer.identify_input_type(json_content)\n",
    "\n",
    "# print(f\"HTML file type: {html_type}\")\n",
    "# print(f\"XML file type: {xml_type}\")\n",
    "# print(f\"JSON file type: {json_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html>\\n<head>\\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\">\\n    <meta name=\"viewport\" content=\"width=1920\"/>\\n    <meta name=\"description\"\\n          content=\"Tizen Download API Demo\" />\\n\\n    <title>Tizen Download API</title>\\n\\n    <script type=\\'text/javascript\\' src=\\'$WEBAPIS/webapis/webapis.js\\'></script>\\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\" />\\n\\n</head>\\n\\n<body>\\n\\n    <h1>Tizen Download API</h1>\\n\\n    <div class=\"left\">\\n        <h2>Available buttons/actions:</h2>\\n\\n        <div>\\n            1 - Downloaded file lists<br>\\n            2 - Start download - small file<br>\\n            3 - Start download - large file<br>\\n            Pause - Pause download<br>\\n            Play - Resume download<br>\\n            Stop - Cancel download<br>\\n            0 - Clear logs\\n        </div>\\n\\n        <div id=\"progress\" class=\"progress\">\\n            <div id=\"bar\" class=\"bar\"></div>\\n        </div>\\n\\n        <fieldset>\\n            <legend>Downloaded files:</legend>\\n            <div id=\"fileList\" class=\"logs\"></div>\\n        </fieldset>\\n\\n        <fieldset>\\n            <legend>Logs</legend>\\n            <div id=\"logs\" class=\"logs\"></div>\\n        </fieldset>\\n    </div>\\n    <div class=\"right\">\\n        <h2>Info</h2>\\n\\n        <p>This application demonstrates the usage of Tizen download API</p>\\n\\n        <p>The application downloads files from <code>http://techslides.com</code> and <code>http://download.blender.org</code>. If these domains are not accessible it will not work.<br>\\n\\n            You can change the source location of files to download by changing the contents of <code>url1</code> and <code>url2</code> variables in <code>main.js</code> file</cide></p>\\n    </div>\\n\\n    <script src=\"main.js\"></script>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydantic import BaseModel, Field\n",
    "# from typing import List, Dict, Union, Optional\n",
    "# from openai import OpenAI\n",
    "\n",
    "# client = OpenAI()\n",
    "# MODEL = \"gpt-3.5-turbo\"  # or your preferred model\n",
    "\n",
    "# class Metadata(BaseModel):\n",
    "#     tag: Optional[str] = None\n",
    "#     attributes: Optional[str] = None\n",
    "#     formatting: Optional[str] = None\n",
    "\n",
    "# class TranslatableElement(BaseModel):\n",
    "#     type: str\n",
    "#     content: str\n",
    "#     metadata: Optional[Metadata] = None\n",
    "#     non_translatable: List[str] = Field(default_factory=list)\n",
    "\n",
    "# class TranslatableContent(BaseModel):\n",
    "#     elements: List[TranslatableElement]\n",
    "\n",
    "# def extract_translatable_content(content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "#     prompt = f\"\"\"\n",
    "#         Analyze and extract the translatable content from the following {input_type} input.\n",
    "#         Return the content as a structured list of elements, where each element represents a translatable item.\n",
    "\n",
    "#         Rules:\n",
    "#         1. Preserve the structure of the original {input_type} input.\n",
    "#         2. Identify and mark domain-specific terms or technical jargon as non-translatable.\n",
    "#         3. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "#         4. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "#         Input: {content}\n",
    "\n",
    "#         Ensure the output follows the structure defined in the TranslatableContent model.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     completion = client.chat.completions.create(\n",
    "#         model=MODEL,\n",
    "#         messages=[\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts translatable content from various input types.\"},\n",
    "#             {\"role\": \"user\", \"content\": prompt},\n",
    "#         ],\n",
    "#         response_model=TranslatableContent,\n",
    "#     )\n",
    "\n",
    "#     return [element.model_dump(exclude_none=True) for element in completion.choices[0].message.elements]\n",
    "\n",
    "# # Example usage\n",
    "# input_content = \"\"\"\n",
    "# <h1>Welcome to our website</h1>\n",
    "# <p>We offer the best <strong>AI-powered</strong> solutions for your business.</p>\n",
    "# <ul>\n",
    "#     <li>Machine Learning</li>\n",
    "#     <li>Natural Language Processing</li>\n",
    "#     <li>Computer Vision</li>\n",
    "# </ul>\n",
    "# \"\"\"\n",
    "\n",
    "# result = extract_translatable_content(input_content, \"HTML\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_translatable_content(content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "    prompt = f\"\"\"\n",
    "        Analyze and extract the translatable content from the following {input_type} input.\n",
    "        Return the content as a structured list of elements, where each element represents a translatable item.\n",
    "\n",
    "        Rules:\n",
    "        1. Preserve the structure of the original {input_type} input.\n",
    "        2. Identify and mark domain-specific terms or technical jargon as non-translatable.\n",
    "        3. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "        4. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "        Input: {content}\n",
    "\n",
    "        Ensure the output follows the structure defined in the TranslatableContent model.\n",
    "    \"\"\"\n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts translatable content from various input types.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        response_model=TranslatableContent,\n",
    "    )\n",
    "\n",
    "    return [element.model_dump(exclude_none=True) for element in completion.choices[0].message.elements]\n",
    "\n",
    "# Example usage for different input types\n",
    "html_input = \"<h1>Welcome</h1><p>This is a paragraph.</p>\"\n",
    "xml_input = \"<root><item>XML content</item></root>\"\n",
    "json_input = '{\"key\": \"JSON value\", \"nested\": {\"subkey\": \"Nested value\"}}'\n",
    "plain_text_input = \"This is plain text.\\nIt has multiple lines.\"\n",
    "\n",
    "print(extract_translatable_content(html_input, \"HTML\"))\n",
    "print(extract_translatable_content(xml_input, \"XML\"))\n",
    "print(extract_translatable_content(json_input, \"JSON\"))\n",
    "print(extract_translatable_content(plain_text_input, \"plain text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Dict, Union\n",
    "# from pydantic import BaseModel, Field\n",
    "# from azure.openai import AzureOpenAI\n",
    "# import os\n",
    "\n",
    "# # Azure OpenAI settings\n",
    "# azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "# api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "# deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# # Initialize Azure OpenAI client\n",
    "# client = AzureOpenAI(\n",
    "#     api_key=api_key,\n",
    "#     api_version=\"2023-05-15\",\n",
    "#     azure_endpoint=azure_endpoint\n",
    "# )\n",
    "\n",
    "# class Metadata(BaseModel):\n",
    "#     tag: str = None\n",
    "#     attributes: str = None\n",
    "#     formatting: str = None\n",
    "\n",
    "# class TranslatableElement(BaseModel):\n",
    "#     type: str\n",
    "#     content: str\n",
    "#     metadata: Metadata = None\n",
    "#     non_translatable: List[str] = Field(default_factory=list)\n",
    "\n",
    "# class TranslatableContent(BaseModel):\n",
    "#     elements: List[TranslatableElement]\n",
    "\n",
    "# def extract_translatable_content(content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "#     prompt = f\"\"\"\n",
    "#         Analyze and extract the translatable content from the following {input_type} input.\n",
    "#         Return the content as a structured list of elements, where each element represents a translatable item.\n",
    "\n",
    "#         Rules:\n",
    "#         1. Preserve the structure of the original {input_type} input.\n",
    "#         2. Identify and mark domain-specific terms or technical jargon as non-translatable.\n",
    "#         3. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "#         4. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "#         Input: {content}\n",
    "\n",
    "#         Ensure the output follows the structure defined in the TranslatableContent model.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     try:\n",
    "#         response = client.chat.completions.create(\n",
    "#             model=deployment_name,  # Use the deployment name here\n",
    "#             messages=[\n",
    "#                 {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts translatable content from various input types.\"},\n",
    "#                 {\"role\": \"user\", \"content\": prompt},\n",
    "#             ],\n",
    "#             response_format={\"type\": \"json_object\"}\n",
    "#         )\n",
    "        \n",
    "#         # Parse the JSON response\n",
    "#         result = TranslatableContent.parse_raw(response.choices[0].message.content)\n",
    "#         return [element.model_dump(exclude_none=True) for element in result.elements]\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error occurred: {str(e)}\")\n",
    "#         return []\n",
    "\n",
    "# # Example usage\n",
    "# html_input = \"<h1>Welcome to Azure</h1><p>This is a cloud service.</p>\"\n",
    "# result = extract_translatable_content(html_input, \"HTML\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from azure.openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Azure OpenAI settings\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    tag: str = None\n",
    "    attributes: str = None\n",
    "    formatting: str = None\n",
    "\n",
    "class TranslatableElement(BaseModel):\n",
    "    type: str\n",
    "    content: str\n",
    "    metadata: Metadata = None\n",
    "    non_translatable: List[str] = Field(default_factory=list)\n",
    "\n",
    "class TranslatableContent(BaseModel):\n",
    "    elements: List[TranslatableElement]\n",
    "\n",
    "def extract_translatable_content(content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "    prompt = f\"\"\"\n",
    "        Analyze and extract the translatable content from the following {input_type} input.\n",
    "        Return the content as a structured list of elements, where each element represents a translatable item.\n",
    "\n",
    "        Rules:\n",
    "        1. Preserve the structure of the original {input_type} input.\n",
    "        2. Identify and mark domain-specific terms or technical jargon as non-translatable.\n",
    "        3. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "        4. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "        Input: {content}\n",
    "\n",
    "        Ensure the output follows the structure defined in the TranslatableContent model.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment_name,  # Use the deployment name here\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts translatable content from various input types.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_model=TranslatableContent\n",
    "        )\n",
    "        \n",
    "        return [element.model_dump(exclude_none=True) for element in completion.choices[0].message.elements]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "html_input = \"<h1>Welcome to Azure</h1><p>This is a cloud service.</p>\"\n",
    "result = extract_translatable_content(html_input, \"HTML\")\n",
    "print(result)\n",
    "\n",
    "\n",
    "def extract_content(data):\n",
    "    \"\"\"\n",
    "    Extract only the 'content' field from the input data.\n",
    "\n",
    "    Args:\n",
    "    data (list): A list of dictionaries, each containing a 'content' key.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of strings, each string being the 'content' value.\n",
    "    \"\"\"\n",
    "    return [item['content'] for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "from azure.openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Azure OpenAI setup\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "class InputTypeResponse(BaseModel):\n",
    "    input_type: Literal['html', 'xml', 'json', 'markdown', 'yaml', 'csv', 'plaintext'] = Field(\n",
    "        ...,\n",
    "        description=\"The identified type of the input content\"\n",
    "    )\n",
    "\n",
    "class ContentAnalyzer:\n",
    "    def identify_input_type(self, content: str) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following content and identify its type. Respond with a single word in lowercase, choosing from:\n",
    "        - html\n",
    "        - xml\n",
    "        - json\n",
    "        - markdown\n",
    "        - yaml\n",
    "        - csv\n",
    "        - plaintext\n",
    "\n",
    "        Use 'plaintext' if the content doesn't match any specific format.\n",
    "        \n",
    "        Guidelines:\n",
    "        - Look for distinctive markers like HTML tags, XML declarations, JSON brackets, or Markdown syntax.\n",
    "        - Consider structure and formatting, not just the presence of certain characters.\n",
    "        - If multiple formats are present, choose the predominant one.\n",
    "\n",
    "        Content: {content[:1000]}\n",
    "\n",
    "        Respond only with the type, nothing else.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a content type analyzer.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_model=InputTypeResponse\n",
    "            )\n",
    "            \n",
    "            return completion.choices[0].message.input_type\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            return \"unknown\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer = ContentAnalyzer()\n",
    "    \n",
    "    test_contents = [\n",
    "        \"<html><body><h1>Hello</h1></body></html>\",\n",
    "        \"<?xml version='1.0'?><root><element>Content</element></root>\",\n",
    "        '{\"key\": \"value\", \"array\": [1, 2, 3]}',\n",
    "        \"# Markdown Header\\n\\nThis is some markdown content.\",\n",
    "        \"key: value\\nnested:\\n  subkey: subvalue\",\n",
    "        \"column1,column2,column3\\nvalue1,value2,value3\",\n",
    "        \"Just some plain text content.\"\n",
    "    ]\n",
    "\n",
    "    for content in test_contents:\n",
    "        input_type = analyzer.identify_input_type(content)\n",
    "        print(f\"Identified type: {input_type}\")\n",
    "        print(f\"For content: {content[:50]}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Dict, Union\n",
    "from azure.openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Azure OpenAI setup\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_KEY\"),\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "class ReconstructedContent(BaseModel):\n",
    "    content: str = Field(..., description=\"The reconstructed content in its original format\")\n",
    "    input_type: str = Field(..., description=\"The type of the input/output content (e.g., 'html', 'xml', 'json')\")\n",
    "\n",
    "class ContentReconstructor:\n",
    "    def reconstruct_output(\n",
    "        self, \n",
    "        original_content: str, \n",
    "        extracted_structure: List[Dict[str, Union[str, List[str]]]], \n",
    "        translated_content: List[str], \n",
    "        input_type: str\n",
    "    ) -> str:\n",
    "        prompt = f\"\"\"\n",
    "        Reconstruct the original {input_type} format using the translated content and extracted structure.\n",
    "\n",
    "        Original content: \n",
    "        {original_content}  # Limiting to first 1000 characters for brevity\n",
    "\n",
    "        Extracted structure:\n",
    "        {extracted_structure}\n",
    "\n",
    "        Translated content (in order of extraction):\n",
    "        {translated_content}\n",
    "\n",
    "        Instructions:\n",
    "        1. Use the extracted structure to guide the reconstruction process.\n",
    "        2. Replace the original text in each extracted element with the corresponding translated text.\n",
    "        3. Preserve all original formatting, tags, attributes, and non-translatable content.\n",
    "        4. Ensure that the reconstructed content maintains the same structure and order as the original.\n",
    "        5. For any domain-specific terms, technical terms or proper nouns that were marked as non-translatable, use the original text.\n",
    "        6. If there are any placeholders or variables in the original content, ensure they are correctly maintained in the translated version.\n",
    "\n",
    "        Return the fully reconstructed {input_type} content, ensuring it's a valid and well-formatted {input_type} document.\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a content reconstruction specialist.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_model=ReconstructedContent\n",
    "            )\n",
    "            \n",
    "            result = completion.choices[0].message\n",
    "            return result.content  # Return just the content string\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred: {str(e)}\")\n",
    "            return \"\"\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    reconstructor = ContentReconstructor()\n",
    "    \n",
    "    # Example inputs (you would replace these with your actual data)\n",
    "    original_content = \"<html><body><h1>Hello</h1><p>World</p></body></html>\"\n",
    "    extracted_structure = [\n",
    "        {\"type\": \"heading\", \"content\": \"Hello\", \"metadata\": {\"tag\": \"h1\"}},\n",
    "        {\"type\": \"paragraph\", \"content\": \"World\", \"metadata\": {\"tag\": \"p\"}}\n",
    "    ]\n",
    "    translated_content = [\"Bonjour\", \"le monde\"]\n",
    "    input_type = \"html\"\n",
    "\n",
    "    reconstructed = reconstructor.reconstruct_output(\n",
    "        original_content, \n",
    "        extracted_structure, \n",
    "        translated_content, \n",
    "        input_type\n",
    "    )\n",
    "\n",
    "    print(\"Reconstructed content:\")\n",
    "    print(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Union\n",
    "from pydantic import BaseModel, Field\n",
    "from azure.openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Azure OpenAI settings\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "client = AzureOpenAI(\n",
    "    api_key=api_key,\n",
    "    api_version=\"2023-05-15\",\n",
    "    azure_endpoint=azure_endpoint\n",
    ")\n",
    "\n",
    "class Metadata(BaseModel):\n",
    "    tag: str = None\n",
    "    attributes: str = None\n",
    "    formatting: str = None\n",
    "\n",
    "class TranslatableElement(BaseModel):\n",
    "    type: str\n",
    "    content: str\n",
    "    metadata: Metadata = None\n",
    "    non_translatable: List[str] = Field(default_factory=list)\n",
    "\n",
    "class TranslatableContent(BaseModel):\n",
    "    elements: List[TranslatableElement]\n",
    "\n",
    "def load_rules(file_path: str = 'translation_rules.json') -> Dict:\n",
    "    \"\"\"Load translation rules from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Rules file not found: {file_path}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Invalid JSON in rules file: {file_path}\")\n",
    "        return {}\n",
    "\n",
    "def extract_translatable_content(content: str, input_type: str) -> List[Dict[str, Union[str, List[str]]]]:\n",
    "    # Load rules\n",
    "    rules = load_rules()\n",
    "    \n",
    "    # Construct prompt with loaded rules\n",
    "    rules_text = \"\\n\".join(rules.get('extraction_rules', []))\n",
    "    prompt = f\"\"\"\n",
    "        Analyze and extract the translatable content from the following {input_type} input.\n",
    "        Return the content as a structured list of elements, where each element represents a translatable item.\n",
    "\n",
    "        Rules:\n",
    "        {rules_text}\n",
    "\n",
    "        Additional instructions:\n",
    "        1. Preserve the structure of the original {input_type} input.\n",
    "        2. Include relevant metadata to aid in reconstructing the original format after translation.\n",
    "        3. For plain text input, use \"paragraph\" as the type and omit the metadata.\n",
    "\n",
    "        Input: {content}\n",
    "\n",
    "        Ensure the output follows the structure defined in the TranslatableContent model.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=deployment_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant that extracts translatable content from various input types.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            response_model=TranslatableContent\n",
    "        )\n",
    "        \n",
    "        return [element.model_dump(exclude_none=True) for element in completion.choices[0].message.elements]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    html_input = \"<h1>Welcome to Azure</h1><p>This is a cloud service.</p>\"\n",
    "    result = extract_translatable_content(html_input, \"HTML\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
